{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installing required libraries\n",
    "%pip install --quiet -r requirements.txt #ditching faiss for now and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading environment variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\") # Load using .env file\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace Login\n",
    "from huggingface_hub import login\n",
    "login(token = HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports of each section should MUST be self contained to make follow-up modulization efforts easier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main inference LLM \n",
    "We will use GroqCloud for now, but will eventually be swapping to a self-hosted model, [documentation](https://dspy-docs.vercel.app/docs/building-blocks/language_models#remote-lms)\n",
    "\n",
    "TODO: add an \"infernece\" function to abstract away the implementation (since we might swap providers etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud inference (\"Normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from groq import Groq\n",
    "\n",
    "# client = Groq(api_key = GROQ_API_KEY)\n",
    "\n",
    "groq_args = {\n",
    "    \"model\": \"llama-3.1-8b-instant\",\n",
    "    \"max_tokens\": 8192,\n",
    "    \"temperature\": 0.2, # To tune\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DSPy\n",
    "\n",
    "Abandoned for now, DSPy requires a dataset to \"optimize\" the prompts, we do not have a multiround multihop dataset yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dspy\n",
    "\n",
    "# groq_args = {\n",
    "#     \"api_key\": GROQ_API_KEY,\n",
    "#     \"model\": \"llama-3.1-8b-instant\",\n",
    "#     \"max_tokens\": 8192,\n",
    "#     \"temperature\": 0.2, # To tune\n",
    "# }\n",
    "\n",
    "# groq = dspy.GROQ(**groq_args)\n",
    "# dspy.settings.configure(lm=groq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Inference\n",
    "Replace cloud inference part with following code\n",
    "\n",
    "Regarding estimation of token count, we will use the tokenizer from the embedding model during demo, but in production, we will use llama's tokenizer instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Imports\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# # Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding model\n",
    "\n",
    "See [huggingface mteb leaderboards](https://huggingface.co/spaces/mteb/leaderboard)\n",
    "\n",
    "As of the creation of the notebook (15/7/24), the best model is \"dunzhang/stella_en_1.5B_v5\" (mit licence, so we can use it commercially)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Select embedding model\n",
    "embedding_model_name = \"dunzhang/stella_en_1.5B_v5\"\n",
    "\n",
    "# Load embedding model\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "embedding_model = AutoModel.from_pretrained(embedding_model_name).to('cuda')\n",
    "\n",
    "# get_embeddings function using Dependancy injection\n",
    "def get_embeddings(texts, embedding_tokenizer, embedding_model):\n",
    "    inputs = embedding_tokenizer(texts, return_tensors='pt', padding=True, truncation=True).to('cuda')\n",
    "    with torch.no_grad():\n",
    "        embeddings = embedding_model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking model\n",
    "\n",
    "In the [Fudan RAG review paper](https://arxiv.org/abs/2407.01219), it is shown that MonoT5 has the best performance/ latency tradeoff. \n",
    "\n",
    "We opt to use a fine tuned version of MonoT5 (castorini/monot5-base-msmarco-10k), (we have requested licencing information and will update this after we get a response) \n",
    "\n",
    "TODO: Figure out how to rerank properly\n",
    "TODO: When changing this, remember to also change Pool of Queries Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Select reranker model\n",
    "reranker_model_name = \"castorini/monot5-base-msmarco-10k\"\n",
    "\n",
    "# Load reranker model\n",
    "reranker_tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)\n",
    "reranker_model = AutoModelForSeq2SeqLM.from_pretrained(reranker_model_name).to('cuda')\n",
    "\n",
    "# Get rank function\n",
    "# Use case:\n",
    "# chunks_text = [*a list of retrieved text chunks*]\n",
    "# reranked_indices = get_ranks(chunks_text, reranker_tokenizer, reranked_indices)\n",
    "# top_chunks = [chunks_text[i] for i in reranked_indices[:3]]\n",
    "def get_ranks(query, chunks_text, reranker_tokenizer, reranker_model):\n",
    "    # Prepare input by combining query with each chunk\n",
    "    input_texts = [f\"Query: {query} Document: {chunk}\" for chunk in chunks_text]\n",
    "    # Tokenize inputs\n",
    "    inputs = reranker_tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to('cuda')\n",
    "    # Generate scores\n",
    "    with torch.no_grad():\n",
    "        outputs = reranker_model.generate(**inputs, max_length=20, num_return_sequences=1, output_scores=True, return_dict_in_generate=True)\n",
    "        scores = outputs.sequences_scores\n",
    "    # Get ranked indices\n",
    "    reranked_indices = torch.argsort(scores, descending=True).cpu().numpy()\n",
    "    return reranked_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Loader\n",
    "\n",
    "We use Unstructured to load files and provide a helper function to traverse through a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from unstructured.partition.auto import partition\n",
    "\n",
    "def read_file(file_path):\n",
    "    # Read a file using Unstructured library.\n",
    "    elements = partition(filename=file_path)\n",
    "    # Process elements as needed, e.g., extract text\n",
    "    return ' '.join([el.text for el in elements])\n",
    "\n",
    "def read_directory(directory_path, recursive=True):\n",
    "    # Recursively traverse directory and read files.\n",
    "    documents = []\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                content = read_file(file_path)\n",
    "                documents.append({\"content\": content, \"source\": file_path})\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {file_path}: {e}\")\n",
    "        \n",
    "        if not recursive:\n",
    "            break  # Don't process subdirectories if recursive is False\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "For now, we use nltk punkt model to perform sentence level chunking\n",
    "\n",
    "If sentiment level chunking is cheap enough we use that instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Downloads the punkt model\n",
    "nltk.download('punkt')\n",
    "\n",
    "# WARNING: Assumes \"Languages with romanic characters\" e.g. English French Spanish etc only, DOES NOT WORK WITH CHINESE/ JAPANESE/ KOREAN etc\n",
    "# Over-engineering go crazy, the exact token count doesnt matter much anyways because the embedding model can use a different embedding compared to the space anyways\n",
    "# Returns a list of dictionaries with members: \"text\", \"chunk_length\"\n",
    "def sentence_level_chunking(text, chunk_size = 256, embedding_tokenizer = None, estimate_token_count: bool = False, token_per_word_ratio = 0.75):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    # This looks funny because \"sentences\" is not a list and can only assume iterator properties + exception case to handle long sentences\n",
    "    for sentence in sentences:\n",
    "        to_process = [sentence]\n",
    "        while to_process: # At least 1 member in to_process\n",
    "            sentence_length = token_count(to_process[0], embedding_tokenizer, estimate_token_count, token_per_word_ratio)\n",
    "            if current_length + sentence_length <= chunk_size:\n",
    "                current_chunk.append(to_process[0])\n",
    "                current_length += sentence_length\n",
    "            elif sentence_length <= chunk_size:\n",
    "                chunks.append(create_chunk_dict(current_chunk, current_length))\n",
    "                current_chunk = []\n",
    "                to_process.append(sentence) # TODO: Same sentence length is recalculated next iteration, fix it.\n",
    "            else: # sentence_length >= chunk_size, should only be invokes in very rare cases\n",
    "                split_sentences = []\n",
    "                if estimate_token_count:\n",
    "                    split_sentences = split_sentences_estimate_tokencount(sentence, chunk_size, token_per_word_ratio)\n",
    "                else: # estimate_token_count = false\n",
    "                    split_sentences = split_sentences_no_estimation(sentence, sentence_length, chunk_size, embedding_tokenizer)\n",
    "                to_process = to_process.extend(split_sentences)\n",
    "            to_process.pop()\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def token_count(sentence, embedding_tokenizer, estimate_token_count: bool, token_per_word_ratio):\n",
    "    sentence_length = -1\n",
    "    if estimate_token_count:\n",
    "        sentence_length = len(sentence.split())*token_per_word_ratio\n",
    "    else:\n",
    "        try:\n",
    "            sentence_length = len(embedding_tokenizer.tokenize(sentence))\n",
    "        except:\n",
    "            raise TypeError(f\"Embedding_tokenizer is of invalid type: {type(embedding_tokenizer)}\") # I don't like this\n",
    "    return sentence_length\n",
    "\n",
    "def create_chunk_dict(current_chunk, current_length):\n",
    "    chunk_dict = {\n",
    "        \"text\": \" \".join(current_chunk),\n",
    "        \"chunk_length\": current_length,\n",
    "    }\n",
    "    return chunk_dict\n",
    "\n",
    "def split_sentences_estimate_tokencount(sentence, chunk_size, token_per_word_ratio):\n",
    "    split_sentences_words = sentence.split()\n",
    "    words_per_chunk = int(chunk_size * token_per_word_ratio)\n",
    "    split_sentences = [split_sentences_words[i:i+words_per_chunk] for i in range(0, len(split_sentences_words), words_per_chunk)]\n",
    "    split_sentences_string = \" \".join(split_sentences)\n",
    "    return split_sentences_string\n",
    "\n",
    "# Case for no estimation of token_count is bad (since I don't know how to get the thing to select the first {chunk_size} items)\n",
    "# For now, we use the same approach as the \"estimate tokencount\" case, except that we calculate the token per word ratio by using the sentence length obtained from the token_count function\n",
    "def split_sentences_no_estimation(sentence, sentence_length, chunk_size, embedding_tokenizer):\n",
    "    split_sentences_words = sentence.split()\n",
    "    token_per_word_ratio = sentence_length/ len(split_sentences_words)\n",
    "    words_per_chunk = int(chunk_size * token_per_word_ratio)\n",
    "    split_sentences = [split_sentences_words[:words_per_chunk], split_sentences_words[words_per_chunk:]]\n",
    "    split_sentences_string = \" \".join(split_sentences)\n",
    "    return split_sentences_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VectorDB\n",
    "\n",
    "Using Malvus since it is Open Source and has good features\n",
    "\n",
    "Might move to a GraphDB in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType\n",
    "\n",
    "# connections.connect(host='localhost', port='19530')\n",
    "\n",
    "embedding_dims = 1024\n",
    "schema_desc = \"Document collection with chunking information\"\n",
    "\n",
    "# Kwargs:\n",
    "# embedding_dim: the dimensions of the embeddings\n",
    "fields = [\n",
    "    FieldSchema(name=\"document_id\", dtype= DataType.INT64, is_primary=True),\n",
    "    FieldSchema(name=\"chunk_id\", dtype= DataType.INT64),\n",
    "    FieldSchema(name=\"chunk_length\", dtype = DataType.INT64),\n",
    "    FieldSchema(name=\"chunk_text\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "    FieldSchema(name=\"embedding\", dtype= DataType.FLOAT_VECTOR, dim = embedding_dims)\n",
    "]\n",
    "schema = CollectionSchema(fields, description=schema_desc)\n",
    "\n",
    "# collection = Collection(name=\"documents\", schema=schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval function\n",
    "\n",
    "We do this to implement \"padding\", [search param documentation](https://milvus.io/docs/single-vector-search.md#Search-parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import Collection\n",
    "from typing import List\n",
    "\n",
    "# Query\n",
    "def retrieve(embedded_query, top_k_retrieved, collection: Collection, search_params: dict, padding = None) -> List[str]:   \n",
    "    results = retrieve_vector(embedded_query, top_k_retrieved, collection, search_params)\n",
    "    results = results[0] # Returns a single item list for some reason\n",
    "    processed_entities = [process_entity(entity, collection, padding) for entity in results]\n",
    "    return processed_entities\n",
    "    \n",
    "\n",
    "def retrieve_vector(embedded_query, top_k_retrieved, collection: Collection, search_params: dict):\n",
    "    results = collection.search(\n",
    "        data = embedded_query,\n",
    "        anns_field = \"embedding\",\n",
    "        search_params = search_params,\n",
    "        limit = top_k_retrieved,\n",
    "        output_fields = None, # Returns all fields (all implicitly retrievable)\n",
    "    )\n",
    "    return results\n",
    "\n",
    "# process_entity is abstracted out for future modification\n",
    "def process_entity(entity, collection: Collection, padding = None) -> str: # padding should be an iterator of 2 float e.g. [0.5, 0.5]\n",
    "    if padding: # Fuck it we don't do typecheck\n",
    "        lower_bound = entity[\"chunk_id\"] - int(-(-padding[0] // 1)) # int(-(-padding[0] // 1)) Rounds up padding[0]\n",
    "        upper_bound = entity[\"chunk_id\"] + int(-(-padding[1] // 1)) + 1\n",
    "        updated_text = []\n",
    "        oob_lower = False\n",
    "        oob_upper = False\n",
    "        for i in range(lower_bound, upper_bound): \n",
    "            if i == entity[\"chunk_id\"]:\n",
    "                updated_text.append(entity[\"chunk_text\"])\n",
    "                continue\n",
    "            \n",
    "            expr = f\"doc_id == {entity['doc_id']} && chunk_id == {entity['chunk_id']}\"\n",
    "            results = collection.query(\n",
    "            expr=expr,\n",
    "            output_fields=[\"chunk_text\"],\n",
    "            )\n",
    "            \n",
    "            # Check if have results\n",
    "            if results:\n",
    "                updated_text.append(results[0][\"chunk_text\"])\n",
    "            else:\n",
    "                if i == lower_bound:\n",
    "                    oob_lower = True\n",
    "                if i == upper_bound - 1:\n",
    "                    oob_upper = True\n",
    "                if i != lower_bound and i != upper_bound - 1 and not oob_lower:\n",
    "                    raise UserWarning(f\"Previous Chunks are found but chunk {i} is missing\")\n",
    "        \n",
    "        # Truncate edge chunks \n",
    "        start_fraction = padding[0] - int((padding[0] // 1))\n",
    "        end_fraction = padding[1] - int((padding[1] // 1))\n",
    "        if start_fraction != 0 and not oob_lower:\n",
    "            updated_text[0] = truncate(updated_text[0], start_fraction, False)\n",
    "        if end_fraction != 0 and not oob_upper:\n",
    "            updated_text[-1] = truncate(updated_text[-1], start_fraction, True)\n",
    "        \n",
    "        # Join retrieved text\n",
    "        new_entity = entity\n",
    "        new_entity[\"chunk_text\"] = \" \".join(updated_text)\n",
    "        \n",
    "        return new_entity\n",
    "        \n",
    "\n",
    "# Assumes \"Languages with romanic characters\", see chunking section \n",
    "def truncate(text: str, keep_ratio, truncate_end):\n",
    "    if truncate_end:\n",
    "        text_truncated = text[:int(len(text)*(1 - keep_ratio))]\n",
    "        text_truncated = text_truncated.rsplit(\" \", 1) # Prevents returning half a word\n",
    "    else: # Truncates the start\n",
    "        text_truncated = text[int(len(text)*(1 - keep_ratio)):]\n",
    "        text_truncated = text_truncated.split(\" \", 1)\n",
    "    return text_truncated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexer/ \"Load documents into VectorDB\" helper function\n",
    "\n",
    "I don't even know how to call it lmao\n",
    "\n",
    "We currently just use input order as document id (doc_id), but this can be changed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import Collection\n",
    "\n",
    "# use partial to create an embedding function \"embedder\" that eats 1 arguement only and returns an embedding (str -> torch.tensor (or other equivalent class))\n",
    "def index_document(document, collection: Collection, embedder, doc_id: int, chunker_kws: dict):\n",
    "    data = []\n",
    "    chunks = sentence_level_chunking(document, **chunker_kws) # returns list of dicts with fields: \"text\" and \"chunk_length\"\n",
    "    for i, chunk in enumerate(chunks): # Should we consider making this it's own function?\n",
    "        embedding = embedder(chunk[\"text\"])\n",
    "        entity_dict = {\n",
    "            \"document_id\": doc_id,\n",
    "            \"chunk_id\": i,\n",
    "            \"chunk_length\": chunk[\"chunk_length\"],\n",
    "            \"chunk_text\": chunk[\"text\"],\n",
    "            \"embedding\": embedding.tolist(),\n",
    "        }\n",
    "        data.append(entity_dict)\n",
    "    collection.insert(data)\n",
    "    # collection.flush()  # might need to flush in production\n",
    "    \n",
    "# Can customize doc_id later\n",
    "def store_and_embed_documents(documents: list, collection: Collection, embedder, chunker_kws: dict = None):\n",
    "    for i, doc in enumerate(documents):\n",
    "        index_document(doc, collection, embedder, i, chunker_kws)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Processing\n",
    "\n",
    "We store query-context pairs in the following format, for queries obtained from different sources, we differentiate them by placing them into different buckets\n",
    "\n",
    "We code for Groq first, should be able to change the code to suit other providers easily. [tutorial](https://python.useinstructor.com/blog/2024/03/07/open-source-local-structured-output-pydantic-json-openai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair = {\n",
    "# \t\"query\": query,\n",
    "# \t\"context\": list_of_retrieved_context # = [context_1, context_2, ... , context_k]\n",
    "# }\n",
    "\n",
    "# retrieved: list[dict] = [pair_1, pair_2, ..., pair_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get structured response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Type\n",
    "from groq import Groq\n",
    "from pydantic import BaseModel\n",
    "\n",
    "#TODO: Change this to a routing function for other providers\n",
    "def get_response(messages: List[dict], client: Groq, response_model: Type[BaseModel], groq_args: dict, **kwargs):\n",
    "    response = client.chat.completions.create(\n",
    "        response_model = response_model,\n",
    "        messages = messages,\n",
    "        **groq_args\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' (Explain role as Query Classification Module),the chat history between the user and the Large Language model chatbot will be provided below, Based on the last message, is the user asking something currently?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_sysprompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": (\"(Explain role as Query Classification Module),\" \n",
    "                \"the chat history between the user and the Large Language model chatbot\"\n",
    "                \" will be provided below, Based on the last message, \"\n",
    "                \"is the user asking something currently?\") # TODO\n",
    "    } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from groq import Groq\n",
    "from typing import List\n",
    "\n",
    "class Asking(BaseModel):\n",
    "    thoughts: str\n",
    "    response: bool # If the user is asking or not\n",
    "    \n",
    "# def classify(chat_history: List[dict], client: Groq) -> bool:\n",
    "#     client_input = [classify_sysprompt] + chat_history\n",
    "#     response: Asking = client.chat.completions.create(\n",
    "#         response_model = Asking,\n",
    "#         messages = client_input,\n",
    "#         **groq_args\n",
    "#     )\n",
    "#     return response.asking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualify existing pairs (removal of irrelevant pairs)\n",
    "\n",
    "Re-use this for requalifying old pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualify_sysprompt = \"(Needs to coerce the model to generate len(pairs) and only len(pairs) of booleans)\" #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, validator, ValidationError\n",
    "from groq import Groq\n",
    "from typing import List\n",
    "\n",
    "# max_length_per_split: determines the largest number of element per split, value = 0 equals sending the whole list for processing\n",
    "def qualify_all_pairs(pairs: List[dict], chat_history: List[dict], client: Groq, max_length_per_split: int = 0) -> List[bool]:\n",
    "    splitted = split_list(pairs, max_length_per_split)\n",
    "    split_results = [qualify_pairs(pairs_list, chat_history, client) for pairs_list in splitted]\n",
    "    results = [pair for sublist in split_results for pair in sublist]\n",
    "    return results\n",
    "\n",
    "def split_list(pairs: List[dict], max_length_per_split) -> List[List[dict]]:\n",
    "    # Check No splitting\n",
    "    if not max_length_per_split:\n",
    "        return pairs\n",
    "    return [pairs[i:i+max_length_per_split] for i in range(0, len(pairs), max_length_per_split)]\n",
    "    \n",
    "# TODO: Abstract out the qualify_pair function like other functions\n",
    "def qualify_pairs(pairs: List[dict], \n",
    "                  chat_history: List[dict], \n",
    "                  client: Groq) -> List[bool]:\n",
    "    client_input = [qualify_sysprompt] + chat_history + [qualify_prompt(pairs)]\n",
    "    response_model = batch_qualify(len(pairs))\n",
    "    response = get_response(\n",
    "        response_model = response_model,\n",
    "        client = client\n",
    "        messages = client_input,\n",
    "        **groq_args\n",
    "    )\n",
    "    return response.qualify\n",
    "\n",
    "\n",
    "def batch_qualify(batch_length: int):\n",
    "    class BatchQualify(BaseModel):\n",
    "        thoughts: str\n",
    "        qualify: List[bool]\n",
    "        \n",
    "    # Method will be depreciated, need to find the new way to do it properly\n",
    "    @validator(\"qualify\")\n",
    "    def check_length(cls, v):\n",
    "        if len(v) != batch_length:\n",
    "            raise ValueError(f\"Returned length: {len(v)} does not match the number of input pairs: {batch_length}\")\n",
    "    \n",
    "    return BatchQualify\n",
    "    \n",
    "def qualify_prompt(pairs: List[dict]) -> str:\n",
    "    msg = {\n",
    "        \"role\": \"User\",\n",
    "        \"msg\": (f\"Consider the following query-retrieved pairs: {pairs}.\"\n",
    "                \"is it relevant to the user's last message?\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One by One (Fall Back)\n",
    "\n",
    "Use this if batch qualification is not possible/ has poor performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pydantic import BaseModel, validator, ValidationError\n",
    "# from groq import Groq\n",
    "# from typing import List\n",
    "        \n",
    "# def qualify_all_pairs(pairs: List[dict], chat_history: List[dict], client: Groq) -> List[bool]:\n",
    "#     return [qualify_pair(pair, chat_history, client) for pair in pairs]\n",
    "\n",
    "# # Fallback, if system cannot output fixed length list\n",
    "# class Qualify(BaseModel):\n",
    "#     thoughts: str\n",
    "#     qualify: bool\n",
    "\n",
    "# def qualify_pair(pair: dict, chat_history: List[dict], client: Groq) -> bool:\n",
    "#     client_input = [qualify_sysprompt] + chat_history + [qualify_prompt(pair)]\n",
    "#     response: Qualify = client.chat.completions.create(\n",
    "#         response_model = Qualify,\n",
    "#         messages = client_input,\n",
    "#         **groq_args\n",
    "#     )\n",
    "    \n",
    "#     return response.qualify\n",
    "    \n",
    "# def qualify_prompt(pair: dict) -> str:\n",
    "#     msg = {\n",
    "#         \"role\": \"User\",\n",
    "#         \"msg\": (f\"Consider the following query-retrieved pair: {pair}.\"\n",
    "#                 \"is it relevant to the user's last message?\")\n",
    "#     }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qualify_existing_pairs(pairs: List[dict], chat_history: List[dict], client: Groq, max_length_per_split: int = 0):\n",
    "    qualified = qualify_all_pairs(pairs, chat_history, client, max_length_per_split)\n",
    "    return [pair for pair, qual in zip(pairs, qualified) if qual], [pair for pair, qual in zip(pairs, qualified) if not qual]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate new Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_query_sysprompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": (\"(Explain role as Query Generation Module),\" \n",
    "                \"the chat history between the user and the Large Language model chatbot\"\n",
    "                \" will be provided below, and the current query context pairs, retrieved from the RAG system,\"\n",
    "                \"will be provided below. If the current query context pairs is not sufficient, please \"\n",
    "                \"supplement in additional queries below. (Can first type your thoughts on what the current queries are lacking\"\n",
    "                \"and then decide on what other queries should be generated), the new queries are to be used to retrieve from\"\n",
    "                \"a RAG system, so make the queries as 'seperatable' as possible\")\n",
    "    } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from groq import Groq\n",
    "from typing import List\n",
    "\n",
    "class NewQueries(BaseModel):\n",
    "    thoughts: str\n",
    "    response: List[str] # List of new queries\n",
    "\n",
    "\n",
    "#[new_query_sysprompt] + chat_history + [new_query_prompt(pairs)]\n",
    "\n",
    "# def get_new_queries(pairs: List[dict], chat_history: List[dict], client: Groq) -> List[dict]:\n",
    "#     client_input = [new_query_sysprompt] + chat_history + [new_query_prompt(pairs)]\n",
    "#     response: NewQueries = client.chat.completions.create(\n",
    "#         response_model = NewQueries,\n",
    "#         messages = client_input,\n",
    "#         **groq_args\n",
    "#     )\n",
    "#     return response.new_queries\n",
    "\n",
    "def new_query_prompt(pairs: List[dict]) -> dict:\n",
    "    msg = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (f\"The current query-context pairs is as follows: {pairs}\"\n",
    "                    \"Please supplement additional queries for retrieval from the RAG system\")\n",
    "    }\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyDE (generate hypothetical document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde_sysprompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": (\"((Explain role as hyde module), please generate an answer for the provided query,\"\n",
    "                  \" (answer should be clear, concise and queriable), \"\n",
    "                  \"if the query could not be answered,\"\n",
    "                  \" (such as refering to the events after knowledge cutoff, or is something you cannot answer), do the following:\"\n",
    "                  \"if the query refers to something that an 'answer that looks like the real answer' could not be generated \"\n",
    "                  \"e.g. news event happening after knowledge cutoff, fill the 'generate' slot as False and fill the 'response' slot with an empty string ''\"\n",
    "                  \"otherwise, if a 'look-alike' answer could be generated e.g. for technical terms, etc. just try your best to generate a response\"\n",
    "                  \"Of course, if the knowledge is in the model, fill generated as true and provide your answer in 'response'\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from groq import Groq\n",
    "from typing import List\n",
    "\n",
    "class HyDE(BaseModel):\n",
    "    thoughts: str\n",
    "    generate: bool\n",
    "    response: str\n",
    "\n",
    "# Returns a dictionary with the fields \"generate\" and \"response\"\n",
    "def get_HyDE(query: str, client: Groq) -> dict:\n",
    "    client_input = [hyde_sysprompt] + [hyde_prompt(query)]\n",
    "    response: HyDE = get_response(\n",
    "        response_model = HyDE,\n",
    "        client = client,\n",
    "        messages = client_input,\n",
    "        **groq_args\n",
    "    )\n",
    "    response = response.model_dump()\n",
    "    response.pop(\"thoughts\", None)\n",
    "    return response\n",
    "\n",
    "def hyde_prompt(query) -> dict:\n",
    "    msg = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Provided query: {query}\"\n",
    "    }\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualify generated answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualify_generated_sysprompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"(explain module role), check if retrieved response is related to the query itself\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from groq import Groq\n",
    "from typing import List\n",
    "\n",
    "# Im trying to not do the \"Class factory\" thing for\n",
    "class QualifyRetrieved(BaseModel):\n",
    "    thoughts: str\n",
    "    response: bool #Qualify or not\n",
    "    \n",
    "def qualify_retrieved(query: str, retrieved: List[str], client: Groq):\n",
    "    pass\n",
    "    \n",
    "def qualify_retrieved_prompt(query: str, retrieved: List[str]):\n",
    "    msg = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Query: {query}, retrieved context: {retrieved}\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pool of Queries Main\n",
    "\n",
    "Remember to change the get_ranks function when changing the reranks function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, cast, Callable, Any\n",
    "\n",
    "# Notes:\n",
    "# May add a counter to relevant pairs cached to discard pairs that are irrelevant for multiple rounds\n",
    "# for pair qualification\n",
    "\n",
    "class PoolOfQueries():\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        client: Groq,\n",
    "        embedding_function: Callable[[str], Any],\n",
    "        rerank_function: Callable[[List[Any], str], Any], # Takes in a query and the chunks and returns the ranks\n",
    "        top_k_retrieve: int = 10,\n",
    "        top_k_rerank: int = 3,\n",
    "        max_length_per_split: int = 0,\n",
    "        retrieved: List[dict] = None,\n",
    "        retrieved_cached: List[dict] = None,\n",
    "        hypothetical: List[dict] = None,\n",
    "        unanswerable: List[dict] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.client = client\n",
    "        self.embedder = embedding_function\n",
    "        self.reranker = rerank_function\n",
    "        self.retrieved = retrieved or []\n",
    "        self.retrieved_cached = retrieved_cached or []\n",
    "        self.hypothetical = hypothetical or []\n",
    "        self.unanswerable = unanswerable or []\n",
    "        self.max_length_per_split = max_length_per_split\n",
    "        self.top_k_retrieve = top_k_retrieve\n",
    "        self.top_k_rerank = top_k_rerank\n",
    "        \n",
    "    def update(self, messages, collection: Collection, search_params) -> None:\n",
    "        # Query Classification\n",
    "        response: bool = cast(bool, get_response(messages, self.client, Asking).response)\n",
    "        if not response:\n",
    "            return\n",
    "        \n",
    "        # Query-context pairs qualification\n",
    "        # Seperating different type of pairs for live settings\n",
    "        relevant_pairs, irrelevant_pairs = qualify_existing_pairs(self.retrieved, messages, self.client, 4)\n",
    "        relevant_pairs_cached, irrelevant_pairs_cached = qualify_existing_pairs(self.retrieved_cached, messages, self.client, 4)\n",
    "        hypothetical_pairs, _ = qualify_existing_pairs(self.hypothetical, messages, self.client, 4)\n",
    "        unanswerable_queries, _ = qualify_existing_pairs(self.unanswerable, messages, self.client, 4)\n",
    "        \n",
    "        #Updating each bucket\n",
    "        self.retrieved = relevant_pairs + relevant_pairs_cached\n",
    "        self.retrieved_cached = irrelevant_pairs + irrelevant_pairs_cached\n",
    "        self.hypothetical = hypothetical_pairs\n",
    "        self.unanswerable = unanswerable_queries\n",
    "        \n",
    "        # Generation of new queries\n",
    "        msg = [new_query_sysprompt] + messages + [new_query_prompt(self.retrieved + self.hypothetical)]\n",
    "        new_queries: List[str] = get_response(msg, self.client, NewQueries).response\n",
    "        \n",
    "        # Generate HyDE\n",
    "        query_hyde: List[dict] = [get_HyDE(x, self.client) for x in new_queries]\n",
    "        \n",
    "        # Retrieve \n",
    "        retrieve_queries: List[str] = self._retrieve_queries(new_queries, query_hyde)\n",
    "        retrieve_embeddings = [self.embedder(query) for query in retrieve_queries]\n",
    "        retrieved_unranked: List[List[str]] = [retrieve(embedded_query) for embedded_query in retrieve_embeddings]\n",
    "        \n",
    "        #Rerank\n",
    "        retrieved_ranks = [self.reranker(query, chunks) for query, chunks in zip(retrieve_queries, retrieved_unranked)]\n",
    "        retrieved_ranked = [[retrieved_unranked[i] for i in rank[:self.top_k_rerank]] for chunks, rank in zip(retrieved_unranked, retrieved_ranks)]\n",
    "        new_pairs = [self._pair_factory(query, context) for query, context in zip(retrieve_queries, retrieved_ranked)]\n",
    "        \n",
    "        # TODO: Qualify Generated\n",
    "        # Check if retrieved chunks is relevant, if not, consider the generated answer\n",
    "        # If the Model thinks it can answer it, then place the query in the Hypothetical bucket, replacing retrieved content with hypothetical\n",
    "        # If the model thinks it can't answer it, then place the query alone in the Unanswerable bucket\n",
    "        \n",
    "        \n",
    "        # Updating buckets again\n",
    "        self.retrieved.extend(new_pairs)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _retrieve_queries(queries, hydes) -> List[str]:\n",
    "        msg_list = []\n",
    "        for query, hyde in zip(queries, hydes):\n",
    "            msg = f\"Query: {query}\"\n",
    "            if hyde.generate:\n",
    "                msg = msg + f\"Hypothetical Answer: {hydes.response}\"\n",
    "            msg_list.append(msg)\n",
    "        return msg_list\n",
    "        \n",
    "    def _pair_factory(query: str, context: List[str]):\n",
    "        pair = {\n",
    "            \"query\": query,\n",
    "            \"context\": context\n",
    "        }\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatUI\n",
    "\n",
    "This is for demonstration only (a web client will be used in production)\n",
    "\n",
    "TODO: Replace the dummy_inference() function with our own inference function after that is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import random\n",
    "\n",
    "# Dummy function remains the same\n",
    "def dummy_inference(message):\n",
    "    responses = [\n",
    "        \"This is a dummy response.\",\n",
    "        \"I'm just pretending to be smart.\",\n",
    "        \"Here's some generated text for testing purposes.\",\n",
    "        \"Imagine this is a well-thought-out answer.\",\n",
    "    ]\n",
    "    return random.choice(responses)\n",
    "\n",
    "class DemoChatUI:\n",
    "    def __init__(self):\n",
    "        self.chat_history = []\n",
    "        self.output = widgets.Output()\n",
    "        self.text_input = widgets.Text(placeholder='Type your message here...')\n",
    "        self.send_button = widgets.Button(description='Send')\n",
    "        self.role_checkbox = widgets.Checkbox(description='Show roles', value=False)\n",
    "        self.reset_button = widgets.Button(description='Reset Chat')\n",
    "        \n",
    "        self.send_button.on_click(self.on_send)\n",
    "        self.text_input.on_submit(self.on_send)\n",
    "        self.role_checkbox.observe(self.update_chat_display, names='value')\n",
    "        self.reset_button.on_click(self.reset_chat)\n",
    "        \n",
    "        input_box = widgets.HBox([self.text_input, self.send_button])\n",
    "        input_box.layout.display = 'flex'\n",
    "        self.text_input.layout.flex = '1'\n",
    "        \n",
    "        bottom_box = widgets.HBox([self.reset_button, self.role_checkbox])\n",
    "        bottom_box.layout.display = 'flex'\n",
    "        bottom_box.layout.justify_content = 'space-between'\n",
    "        \n",
    "        self.chat_box = widgets.VBox([self.output, input_box, bottom_box])\n",
    "        self.main_output = widgets.Output()\n",
    "        \n",
    "        with self.main_output:\n",
    "            display(self.chat_box)\n",
    "        \n",
    "        display(self.main_output)\n",
    "        \n",
    "    def on_send(self, _):\n",
    "        user_message = self.text_input.value\n",
    "        if user_message.strip():\n",
    "            self.add_message(\"user\", user_message)\n",
    "            self.text_input.value = ''\n",
    "            \n",
    "            assistant_response = dummy_inference(user_message)\n",
    "            self.add_message(\"assistant\", assistant_response)\n",
    "            \n",
    "    def add_message(self, role, content):\n",
    "        self.chat_history.append({\"role\": role, \"content\": content})\n",
    "        self.update_chat_display()\n",
    "        \n",
    "    def update_chat_display(self, _=None):\n",
    "        self.output.clear_output()\n",
    "        with self.output:\n",
    "            for message in self.chat_history:\n",
    "                role = message['role']\n",
    "                content = message['content']\n",
    "                \n",
    "                if role == 'user':\n",
    "                    align = 'right'\n",
    "                    color = '#DCF8C6'\n",
    "                elif role == 'assistant':\n",
    "                    align = 'left'\n",
    "                    color = '#E5E5EA'\n",
    "                else:\n",
    "                    align = 'left'\n",
    "                    color = '#F3E5F5'\n",
    "                \n",
    "                role_display = f\"<small>{role}: </small>\" if self.role_checkbox.value else \"\"\n",
    "                \n",
    "                display(HTML(f\"\"\"\n",
    "                    <div style=\"text-align: {align};\">\n",
    "                        <div style=\"display: inline-block; background-color: {color}; padding: 5px 10px; border-radius: 10px; max-width: 70%;\">\n",
    "                            {role_display}{content}\n",
    "                        </div>\n",
    "                    </div>\n",
    "                \"\"\"))\n",
    "\n",
    "    def reset_chat(self, _):\n",
    "        self.chat_history = []\n",
    "        self.text_input.value = ''\n",
    "        self.role_checkbox.value = False\n",
    "        self.update_chat_display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We retrieve more chunks then we acturally provide the LLM\n",
    "top_k_retrieve = 10\n",
    "top_k_reranked = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "\n",
    "documents_path = os.path.join(os.getcwd(), \"documents\") # For demo purposes can only assume \"documents\" is in root directory\n",
    "\n",
    "# Embedding function from get_embeddings\n",
    "embedder = partial(get_embeddings, embedding_tokenizer = embedding_tokenizer, embedding_model = embedding_model)\n",
    "\n",
    "# Setting up VectorDB (milvus)\n",
    "# The collection's schema \"schema\" is defined in the DB portion of the field\n",
    "connections.connect(host='localhost', port='19530')\n",
    "collection = Collection(name=\"documents\", schema=schema)\n",
    "\n",
    "# Main Workflow\n",
    "documents = read_directory(documents_path, recursive = True)\n",
    "store_and_embed_documents(documents, collection, embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ui = DemoChatUI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, validator, ValidationError\n",
    "from typing import List\n",
    "\n",
    "def create_user_extract_model(list_length: int):\n",
    "    class UserExtract(BaseModel):\n",
    "        name: str\n",
    "        age: int\n",
    "        fixed_length_list: List[int]  # Assuming you want a list of integers\n",
    "\n",
    "        @validator('fixed_length_list')\n",
    "        def check_fixed_length(cls, v):\n",
    "            if len(v) != list_length:\n",
    "                raise ValueError(f'fixed_length_list must be of length {list_length}')\n",
    "            return v\n",
    "    \n",
    "    return UserExtract"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
