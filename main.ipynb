{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installing required libraries\n",
    "%pip install --quiet -r requirements.txt #ditching faiss for now and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading environment variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\") # Load using .env file\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace Login\n",
    "from huggingface_hub import login\n",
    "login(token = HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports of each section should MUST be self contained to make follow-up modulization efforts easier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main inference LLM \n",
    "We will use GroqCloud for now, but will eventually be swapping to a self-hosted model, [documentation](https://dspy-docs.vercel.app/docs/building-blocks/language_models#remote-lms)\n",
    "\n",
    "TODO: add an \"infernece\" function to abstract away the implementation (since we might swap providers etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a response\n",
    "\n",
    "We use Llama 3.1 on GroqCloud for now, we will later make it a routing function to support different providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Type\n",
    "from groq import Groq\n",
    "from pydantic import BaseModel\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "\n",
    "#TODO: Change this to a routing function for other providers\n",
    "\n",
    "# \"Normal\" text response\n",
    "@sleep_and_retry\n",
    "@limits(calls = 1, period = 0.5)\n",
    "def get_response(messages: List[dict], client: Groq, groq_args: dict, **kwargs):\n",
    "    response = client.chat.completions.create(\n",
    "        messages = messages,\n",
    "        **groq_args\n",
    "    )\n",
    "    response_dict = dict(response.choices[0].message)\n",
    "    del response_dict['function_call']\n",
    "    del response_dict['tool_calls']\n",
    "    return response_dict\n",
    "\n",
    "# Structured Response\n",
    "# Avoids exceeding call limit\n",
    "@sleep_and_retry\n",
    "@limits(calls = 1, period = 0.5)\n",
    "def get_structured_response(messages: List[dict], \n",
    "                            client: Groq,\n",
    "                            groq_args: dict, \n",
    "                            response_model: Type[BaseModel] = None,\n",
    "                            return_fields: List[str]| str | None = [\"response\"], \n",
    "                            single_item_list_return_dict: bool = False,\n",
    "                            **kwargs):\n",
    "    response = client.chat.completions.create(\n",
    "        response_model = response_model,\n",
    "        messages = messages,\n",
    "        **groq_args\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "def response_fields(response: BaseModel, return_fields: List[str]| str | None, single_item_list_return_dict: bool):\n",
    "    if return_fields is None or len(return_fields) == 0:\n",
    "        return response\n",
    "\n",
    "    if isinstance(return_fields, str):\n",
    "        return getattr(response, return_fields)\n",
    "    \n",
    "    if len(return_fields) == 1 and not single_item_list_return_dict:\n",
    "        return getattr(response, return_fields[0])\n",
    "    \n",
    "    return {field: getattr(response, field) for field in return_fields}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DSPy\n",
    "\n",
    "Abandoned for now, DSPy requires a dataset to \"optimize\" the prompts, we do not have a multiround multihop dataset yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dspy\n",
    "\n",
    "# groq_args = {\n",
    "#     \"api_key\": GROQ_API_KEY,\n",
    "#     \"model\": \"llama-3.1-8b-instant\",\n",
    "#     \"max_tokens\": 8192,\n",
    "#     \"temperature\": 0.2, # To tune\n",
    "# }\n",
    "\n",
    "# groq = dspy.GROQ(**groq_args)\n",
    "# dspy.settings.configure(lm=groq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Inference\n",
    "Replace cloud inference part with following code\n",
    "\n",
    "Regarding estimation of token count, we will use the tokenizer from the embedding model during demo, but in production, we will use llama's tokenizer instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Imports\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# # Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding model\n",
    "\n",
    "See [huggingface mteb leaderboards](https://huggingface.co/spaces/mteb/leaderboard)\n",
    "\n",
    "As of the creation of the notebook (15/7/24), the best model is \"dunzhang/stella_en_1.5B_v5\" (mit licence, so we can use it commercially)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Select embedding model\n",
    "embedding_model_name = \"dunzhang/stella_en_1.5B_v5\"\n",
    "\n",
    "# # Load embedding model\n",
    "# embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "# embedding_model = AutoModel.from_pretrained(embedding_model_name).to('cuda')\n",
    "\n",
    "# get_embeddings function using Dependancy injection\n",
    "def get_embeddings(texts, embedding_tokenizer, embedding_model):\n",
    "    inputs = embedding_tokenizer(texts, return_tensors='pt', padding=True, truncation=True).to('cuda')\n",
    "    embedding_model.eval()\n",
    "    with torch.no_grad():\n",
    "        embeddings = embedding_model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking model\n",
    "\n",
    "In the [Fudan RAG review paper](https://arxiv.org/abs/2407.01219), it is shown that MonoT5 has the best performance/ latency tradeoff. \n",
    "\n",
    "We opt to use a fine tuned version of MonoT5 (castorini/monot5-base-msmarco-10k), (we have requested licencing information and will update this after we get a response) \n",
    "\n",
    "TODO: This method of reranking is WRONG, gonna figure out how to do it correctly when devv.ai llama405b quota is up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Select reranker model\n",
    "reranker_model_name = \"castorini/monot5-base-msmarco-10k\"\n",
    "\n",
    "# Load reranker model\n",
    "reranker_tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)\n",
    "reranker_model = AutoModelForSeq2SeqLM.from_pretrained(reranker_model_name).to('cuda')\n",
    "\n",
    "# Get rank function\n",
    "# Use case:\n",
    "# chunks_text = [*a list of retrieved text chunks*]\n",
    "# reranked_indices = get_ranks(chunks_text, reranker_tokenizer, reranked_indices)\n",
    "# top_chunks = [chunks_text[i] for i in reranked_indices[:3]]\n",
    "# def get_ranks(query, chunks_text, reranker_tokenizer, reranker_model):\n",
    "#     # Prepare input by combining query with each chunk\n",
    "#     input_texts = [f\"Query: {query} Document: {chunk}\" for chunk in chunks_text]\n",
    "#     # Tokenize inputs\n",
    "#     inputs = reranker_tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to('cuda')\n",
    "#     # Generate scores\n",
    "#     with torch.no_grad():\n",
    "#         outputs = reranker_model.generate(**inputs, max_length=20, num_return_sequences=1, output_scores=True, return_dict_in_generate=True)\n",
    "#         scores = outputs.sequences_scores\n",
    "#     # Get ranked indices\n",
    "#     reranked_indices = torch.argsort(scores, descending=True).cpu().numpy()\n",
    "#     return reranked_indices\n",
    "\n",
    "def get_ranks(query, chunks_text, reranker_tokenizer, reranker_model):\n",
    "    # Prepare input by combining query with each chunk\n",
    "    input_texts = [f\"Query: {query} Document: {chunk}\" for chunk in chunks_text]\n",
    "    # Tokenize inputs\n",
    "    inputs = reranker_tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to('cuda')\n",
    "    # Generate scores\n",
    "    with torch.no_grad():\n",
    "        outputs = reranker_model(**inputs)\n",
    "        scores = outputs.logits[:, -1, :]\n",
    "    # Get ranked indices\n",
    "    reranked_indices = torch.argsort(scores, descending=True).cpu().numpy()\n",
    "    return reranked_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Loader\n",
    "\n",
    "We use Unstructured to load files and provide a helper function to traverse through a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from unstructured.partition.auto import partition\n",
    "\n",
    "def read_file(file_path):\n",
    "    # Read a file using Unstructured library.\n",
    "    elements = partition(filename=file_path)\n",
    "    # Process elements as needed, e.g., extract text\n",
    "    return ' '.join([el.text for el in elements])\n",
    "\n",
    "def read_directory(directory_path, recursive=True):\n",
    "    # Recursively traverse directory and read files.\n",
    "    documents = []\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                content = read_file(file_path)\n",
    "                documents.append({\"content\": content, \"source\": file_path})\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {file_path}: {e}\")\n",
    "        \n",
    "        if not recursive:\n",
    "            break  # Don't process subdirectories if recursive is False\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "For now, we use nltk punkt model to perform sentence level chunking\n",
    "\n",
    "If sentiment level chunking is cheap enough we use that instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Downloads the punkt model\n",
    "nltk.download('punkt')\n",
    "\n",
    "# WARNING: Assumes \"Languages with romanic characters\" e.g. English French Spanish etc only, DOES NOT WORK WITH CHINESE/ JAPANESE/ KOREAN etc\n",
    "# Over-engineering go crazy, the exact token count doesnt matter much anyways because the embedding model can use a different embedding compared to the space anyways\n",
    "# Returns a list of dictionaries with members: \"text\", \"chunk_length\"\n",
    "def sentence_level_chunking(text, chunk_size = 256, embedding_tokenizer = None, estimate_token_count: bool = False, token_per_word_ratio = 0.75):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    # This looks funny because \"sentences\" is not a list and can only assume iterator properties + exception case to handle long sentences\n",
    "    for sentence in sentences:\n",
    "        to_process = [sentence]\n",
    "        while to_process: # At least 1 member in to_process\n",
    "            sentence_length = token_count(to_process[0], embedding_tokenizer, estimate_token_count, token_per_word_ratio)\n",
    "            if current_length + sentence_length <= chunk_size:\n",
    "                current_chunk.append(to_process[0])\n",
    "                current_length += sentence_length\n",
    "            elif sentence_length <= chunk_size:\n",
    "                chunks.append(create_chunk_dict(current_chunk, current_length))\n",
    "                current_chunk = []\n",
    "                to_process.append(sentence) # TODO: Same sentence length is recalculated next iteration, fix it.\n",
    "            else: # sentence_length >= chunk_size, should only be invokes in very rare cases\n",
    "                split_sentences = []\n",
    "                if estimate_token_count:\n",
    "                    split_sentences = split_sentences_estimate_tokencount(sentence, chunk_size, token_per_word_ratio)\n",
    "                else: # estimate_token_count = false\n",
    "                    split_sentences = split_sentences_no_estimation(sentence, sentence_length, chunk_size, embedding_tokenizer)\n",
    "                to_process = to_process.extend(split_sentences)\n",
    "            to_process.pop()\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def token_count(sentence, embedding_tokenizer, estimate_token_count: bool, token_per_word_ratio):\n",
    "    sentence_length = -1\n",
    "    if estimate_token_count:\n",
    "        sentence_length = len(sentence.split())*token_per_word_ratio\n",
    "    else:\n",
    "        try:\n",
    "            sentence_length = len(embedding_tokenizer.tokenize(sentence))\n",
    "        except:\n",
    "            raise TypeError(f\"Embedding_tokenizer is of invalid type: {type(embedding_tokenizer)}\") # I don't like this\n",
    "    return sentence_length\n",
    "\n",
    "def create_chunk_dict(current_chunk, current_length):\n",
    "    chunk_dict = {\n",
    "        \"text\": \" \".join(current_chunk),\n",
    "        \"chunk_length\": current_length,\n",
    "    }\n",
    "    return chunk_dict\n",
    "\n",
    "def split_sentences_estimate_tokencount(sentence, chunk_size, token_per_word_ratio):\n",
    "    split_sentences_words = sentence.split()\n",
    "    words_per_chunk = int(chunk_size * token_per_word_ratio)\n",
    "    split_sentences = [split_sentences_words[i:i+words_per_chunk] for i in range(0, len(split_sentences_words), words_per_chunk)]\n",
    "    split_sentences_string = \" \".join(split_sentences)\n",
    "    return split_sentences_string\n",
    "\n",
    "# Case for no estimation of token_count is bad (since I don't know how to get the thing to select the first {chunk_size} items)\n",
    "# For now, we use the same approach as the \"estimate tokencount\" case, except that we calculate the token per word ratio by using the sentence length obtained from the token_count function\n",
    "def split_sentences_no_estimation(sentence, sentence_length, chunk_size, embedding_tokenizer):\n",
    "    split_sentences_words = sentence.split()\n",
    "    token_per_word_ratio = sentence_length/ len(split_sentences_words)\n",
    "    words_per_chunk = int(chunk_size * token_per_word_ratio)\n",
    "    split_sentences = [split_sentences_words[:words_per_chunk], split_sentences_words[words_per_chunk:]]\n",
    "    split_sentences_string = \" \".join(split_sentences)\n",
    "    return split_sentences_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VectorDB\n",
    "\n",
    "Using Malvus since it is Open Source and has good features\n",
    "\n",
    "Might move to a GraphDB in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType\n",
    "\n",
    "# connections.connect(host='localhost', port='19530')\n",
    "\n",
    "embedding_dims = 1024\n",
    "schema_desc = \"Document collection with chunking information\"\n",
    "\n",
    "# Kwargs:\n",
    "# embedding_dim: the dimensions of the embeddings\n",
    "fields = [\n",
    "    FieldSchema(name=\"document_id\", dtype= DataType.INT64, is_primary=True),\n",
    "    FieldSchema(name=\"chunk_id\", dtype= DataType.INT64),\n",
    "    FieldSchema(name=\"chunk_length\", dtype = DataType.INT64),\n",
    "    FieldSchema(name=\"chunk_text\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "    FieldSchema(name=\"embedding\", dtype= DataType.FLOAT_VECTOR, dim = embedding_dims)\n",
    "]\n",
    "schema = CollectionSchema(fields, description=schema_desc)\n",
    "\n",
    "# collection = Collection(name=\"documents\", schema=schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval function\n",
    "\n",
    "We do this to implement \"padding\", [search param documentation](https://milvus.io/docs/single-vector-search.md#Search-parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import Collection\n",
    "from typing import List\n",
    "\n",
    "# Query\n",
    "def retrieve(embedded_query, top_k_retrieved, collection: Collection, search_params: dict, padding = None) -> List[str]:   \n",
    "    results = retrieve_vector(embedded_query, top_k_retrieved, collection, search_params)\n",
    "    results = results[0] # Milvus collection search returns a single item list for some reason\n",
    "    processed_entities = [process_entity(entity, collection, padding) for entity in results]\n",
    "    return processed_entities\n",
    "    \n",
    "\n",
    "def retrieve_vector(embedded_query, top_k_retrieved, collection: Collection, search_params: dict):\n",
    "    results = collection.search(\n",
    "        data = embedded_query,\n",
    "        anns_field = \"embedding\",\n",
    "        search_params = search_params,\n",
    "        limit = top_k_retrieved,\n",
    "        output_fields = None, # Returns all fields (all implicitly retrievable)\n",
    "    )\n",
    "    return results\n",
    "\n",
    "# process_entity is abstracted out for future modification\n",
    "def process_entity(entity, collection: Collection, padding = None) -> str: # padding should be an iterator of 2 float e.g. [0.5, 0.5]\n",
    "    if padding: # Fuck it we don't do typecheck\n",
    "        lower_bound = entity[\"chunk_id\"] - int(-(-padding[0] // 1)) # int(-(-padding[0] // 1)) Rounds up padding[0]\n",
    "        upper_bound = entity[\"chunk_id\"] + int(-(-padding[1] // 1)) + 1\n",
    "        updated_text = []\n",
    "        oob_lower = False\n",
    "        oob_upper = False\n",
    "        for i in range(lower_bound, upper_bound): \n",
    "            if i == entity[\"chunk_id\"]:\n",
    "                updated_text.append(entity[\"chunk_text\"])\n",
    "                continue\n",
    "            \n",
    "            expr = f\"doc_id == {entity['doc_id']} && chunk_id == {entity['chunk_id']}\"\n",
    "            results = collection.query(\n",
    "            expr=expr,\n",
    "            output_fields=[\"chunk_text\"],\n",
    "            )\n",
    "            \n",
    "            # Check if have results\n",
    "            if results:\n",
    "                updated_text.append(results[0][\"chunk_text\"])\n",
    "            else:\n",
    "                if i == lower_bound:\n",
    "                    oob_lower = True\n",
    "                if i == upper_bound - 1:\n",
    "                    oob_upper = True\n",
    "                if i != lower_bound and i != upper_bound - 1 and not oob_lower:\n",
    "                    raise UserWarning(f\"Previous Chunks are found but chunk {i} is missing\")\n",
    "        \n",
    "        # Truncate edge chunks \n",
    "        start_fraction = padding[0] - int((padding[0] // 1))\n",
    "        end_fraction = padding[1] - int((padding[1] // 1))\n",
    "        if start_fraction != 0 and not oob_lower:\n",
    "            updated_text[0] = truncate(updated_text[0], start_fraction, False)\n",
    "        if end_fraction != 0 and not oob_upper:\n",
    "            updated_text[-1] = truncate(updated_text[-1], start_fraction, True)\n",
    "        \n",
    "        # Join retrieved text\n",
    "        new_entity = entity\n",
    "        new_entity[\"chunk_text\"] = \" \".join(updated_text)\n",
    "        \n",
    "        return new_entity\n",
    "        \n",
    "\n",
    "# Assumes \"Languages with romanic characters\", see chunking section \n",
    "def truncate(text: str, keep_ratio, truncate_end):\n",
    "    if truncate_end:\n",
    "        text_truncated = text[:int(len(text)*(1 - keep_ratio))]\n",
    "        text_truncated = text_truncated.rsplit(\" \", 1) # Prevents returning half a word\n",
    "    else: # Truncates the start\n",
    "        text_truncated = text[int(len(text)*(1 - keep_ratio)):]\n",
    "        text_truncated = text_truncated.split(\" \", 1)\n",
    "    return text_truncated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexer/ \"Load documents into VectorDB\" helper function\n",
    "\n",
    "I don't even know how to call it lmao\n",
    "\n",
    "We currently just use input order as document id (doc_id), but this can be changed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import Collection\n",
    "\n",
    "# use partial to create an embedding function \"embedder\" that eats 1 arguement only and returns an embedding (str -> torch.tensor (or other equivalent class))\n",
    "def index_document(document, collection: Collection, embedder, doc_id: int, chunker_kws: dict):\n",
    "    data = []\n",
    "    chunks = sentence_level_chunking(document, **chunker_kws) # returns list of dicts with fields: \"text\" and \"chunk_length\"\n",
    "    for i, chunk in enumerate(chunks): # Should we consider making this it's own function?\n",
    "        embedding = embedder(chunk[\"text\"])\n",
    "        entity_dict = {\n",
    "            \"document_id\": doc_id,\n",
    "            \"chunk_id\": i,\n",
    "            \"chunk_length\": chunk[\"chunk_length\"],\n",
    "            \"chunk_text\": chunk[\"text\"],\n",
    "            \"embedding\": embedding.tolist(),\n",
    "        }\n",
    "        data.append(entity_dict)\n",
    "    collection.insert(data)\n",
    "    # collection.flush()  # might need to flush in production\n",
    "    \n",
    "# Can customize doc_id later\n",
    "def store_and_embed_documents(documents: list, collection: Collection, embedder, chunker_kws: dict = None):\n",
    "    for i, doc in enumerate(documents):\n",
    "        index_document(doc, collection, embedder, i, chunker_kws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Processing\n",
    "\n",
    "We store query-context pairs in the following format, for queries obtained from different sources, we differentiate them by placing them into different buckets\n",
    "\n",
    "We code for Groq first, should be able to change the code to suit other providers easily. [tutorial](https://python.useinstructor.com/blog/2024/03/07/open-source-local-structured-output-pydantic-json-openai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair = {\n",
    "# \t\"query\": query,\n",
    "# \t\"context\": list_of_retrieved_context # = [context_1, context_2, ... , context_k]\n",
    "# }\n",
    "\n",
    "# retrieved: list[dict] = [pair_1, pair_2, ..., pair_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Response models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from groq import Groq\n",
    "from typing import List\n",
    "\n",
    "# Standard models\n",
    "class BooleanModel(BaseModel):\n",
    "    thoughts: str \n",
    "    response: bool # If the user is asking or not\n",
    "    \n",
    "class ListStrModel(BaseModel):\n",
    "    thoughts: str\n",
    "    response: List[str] # List of new queries\n",
    "    \n",
    "    \n",
    "# Custom models\n",
    "class HyDE(BaseModel):\n",
    "    thoughts: str\n",
    "    generate: bool\n",
    "    response: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' (Explain role as Query Classification Module),the chat history between the user and the Large Language model chatbot will be provided below, Based on the last message, is the user asking something currently?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_sysprompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": (\"(Explain role as Query Classification Module),\" \n",
    "                \"the chat history between the user and the Large Language model chatbot\"\n",
    "                \" will be provided below, Based on the last message, \"\n",
    "                \"is the user asking something currently?\") # TODO\n",
    "    }\n",
    "\n",
    "qualify_sysprompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"(Needs to coerce the model to generate len(pairs) and only len(pairs) of booleans)\" #TODO\n",
    "}\n",
    "\n",
    "new_query_sysprompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": (\"(Explain role as Query Generation Module),\" \n",
    "                \"the chat history between the user and the Large Language model chatbot\"\n",
    "                \" will be provided below, and the current query context pairs, retrieved from the RAG system,\"\n",
    "                \"will be provided below. If the current query context pairs is not sufficient, please \"\n",
    "                \"supplement in additional queries below. (Can first type your thoughts on what the current queries are lacking\"\n",
    "                \"and then decide on what other queries should be generated), the new queries are to be used to retrieve from\"\n",
    "                \"a RAG system, so make the queries as 'seperatable' as possible\")\n",
    "    }\n",
    "\n",
    "hyde_sysprompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": (\"((Explain role as hyde module), please generate an answer for the provided query,\"\n",
    "                  \" (answer should be clear, concise and queriable), \"\n",
    "                  \"if the query could not be answered,\"\n",
    "                  \" (such as refering to the events after knowledge cutoff, or is something you cannot answer), do the following:\"\n",
    "                  \"if the query refers to something that an 'answer that looks like the real answer' could not be generated \"\n",
    "                  \"e.g. news event happening after knowledge cutoff, fill the 'generate' slot as False and fill the 'response' slot with an empty string ''\"\n",
    "                  \"otherwise, if a 'look-alike' answer could be generated e.g. for technical terms, etc. just try your best to generate a response\"\n",
    "                  \"Of course, if the knowledge is in the model, fill generated as true and provide your answer in 'response'\")}\n",
    "\n",
    "qualify_generated_sysprompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"(explain module role), check if retrieved response is related to the query itself\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_query_prompt(pairs: List[dict]) -> dict:\n",
    "    msg = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (f\"The current query-context pairs is as follows: {pairs}\"\n",
    "                    \"Please supplement additional queries for retrieval from the RAG system\")\n",
    "    }\n",
    "    return msg\n",
    "\n",
    "def hyde_prompt(query) -> dict:\n",
    "    msg = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Provided query: {query}\"\n",
    "    }\n",
    "    return msg\n",
    "\n",
    "def qualify_retrieved_prompt(query: str, retrieved: List[str]):\n",
    "    msg = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Query: {query}, retrieved context: {retrieved}\"\n",
    "    }\n",
    "    return msg\n",
    "\n",
    "def qualify_prompt(pairs: List[dict]) -> str:\n",
    "    msg = {\n",
    "        \"role\": \"User\",\n",
    "        \"msg\": (f\"Consider the following query-retrieved pairs: {pairs}.\"\n",
    "                \"is it relevant to the user's last message?\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, validator, ValidationError\n",
    "from groq import Groq\n",
    "from typing import Annotated, List\n",
    "from annotated_types import Len\n",
    "\n",
    "# Post processing for qualification of existing pairs\n",
    "def qualify_existing_pairs(pairs: List[dict], chat_history: List[dict], client: Groq, max_length_per_split: int = 0):\n",
    "    qualified = qualify_all_pairs(pairs, chat_history, client, max_length_per_split)\n",
    "    return [pair for pair, qual in zip(pairs, qualified) if qual], [pair for pair, qual in zip(pairs, qualified) if not qual]\n",
    "\n",
    "# max_length_per_split: determines the largest number of element per split, value = 0 equals sending the whole list for processing\n",
    "def qualify_all_pairs(pairs: List[dict], chat_history: List[dict], client: Groq, max_length_per_split: int = 0) -> List[bool]:\n",
    "    splitted = split_list(pairs, max_length_per_split)\n",
    "    split_results = [qualify_pairs(pairs_list, chat_history, client) for pairs_list in splitted]\n",
    "    results = [pair for sublist in split_results for pair in sublist]\n",
    "    return results\n",
    "\n",
    "def split_list(pairs: List[dict], max_length_per_split) -> List[List[dict]]:\n",
    "    # Check No splitting\n",
    "    if not max_length_per_split:\n",
    "        return pairs\n",
    "    return [pairs[i:i+max_length_per_split] for i in range(0, len(pairs), max_length_per_split)]\n",
    "    \n",
    "# TODO: Abstract out the qualify_pair function like other functions\n",
    "def qualify_pairs(pairs: List[dict], \n",
    "                  chat_history: List[dict], \n",
    "                  client: Groq) -> List[bool]:\n",
    "    client_input = [qualify_sysprompt] + chat_history + [qualify_prompt(pairs)]\n",
    "    response_model = batch_qualify(len(pairs))\n",
    "    response = get_structured_response(\n",
    "        response_model = response_model,\n",
    "        client = client,\n",
    "        messages = client_input,\n",
    "        **groq_args\n",
    "    )\n",
    "    return response.qualify\n",
    "\n",
    "\n",
    "def batch_qualify(batch_length: int):\n",
    "    class BatchQualify(BaseModel):\n",
    "        thoughts: str\n",
    "        qualify: Annotated[List[bool], Len(min_length=batch_length, max_length=batch_length)]\n",
    "\n",
    "    return BatchQualify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One by One (Fall Back)\n",
    "\n",
    "Use this if batch qualification is not possible/ has poor performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pydantic import BaseModel, validator, ValidationError\n",
    "# from groq import Groq\n",
    "# from typing import List\n",
    "        \n",
    "# def qualify_all_pairs(pairs: List[dict], chat_history: List[dict], client: Groq) -> List[bool]:\n",
    "#     return [qualify_pair(pair, chat_history, client) for pair in pairs]\n",
    "\n",
    "# # Fallback, if system cannot output fixed length list\n",
    "# class Qualify(BaseModel):\n",
    "#     thoughts: str\n",
    "#     qualify: bool\n",
    "\n",
    "# def qualify_pair(pair: dict, chat_history: List[dict], client: Groq) -> bool:\n",
    "#     client_input = [qualify_sysprompt] + chat_history + [qualify_prompt(pair)]\n",
    "#     response: Qualify = client.chat.completions.create(\n",
    "#         response_model = Qualify,\n",
    "#         messages = client_input,\n",
    "#         **groq_args\n",
    "#     )\n",
    "    \n",
    "#     return response.qualify\n",
    "    \n",
    "# def qualify_prompt(pair: dict) -> str:\n",
    "#     msg = {\n",
    "#         \"role\": \"User\",\n",
    "#         \"msg\": (f\"Consider the following query-retrieved pair: {pair}.\"\n",
    "#                 \"is it relevant to the user's last message?\")\n",
    "#     }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyDE (generate hypothetical document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask if the model wants to generate, if the model doesn't want to generate, just use the original function to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "# Returns a dictionary with the fields \"generate\" and \"response\"\n",
    "def get_HyDE(query: str, client: Groq) -> dict:\n",
    "    client_input = [hyde_sysprompt] + [hyde_prompt(query)]\n",
    "    response: HyDE = get_structured_response(\n",
    "        response_model = HyDE,\n",
    "        client = client,\n",
    "        messages = client_input,\n",
    "        **groq_args\n",
    "    )\n",
    "    response = response.model_dump()\n",
    "    response.pop(\"thoughts\", None)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pool of Queries Main\n",
    "\n",
    "Remember to change the get_ranks function when changing the reranks function\n",
    "\n",
    "Update: Removed \"Hypothetical\" bucket, for unanswerable stuff, we will let the main inference LLM do the work instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, cast, Callable, Any\n",
    "\n",
    "# Notes:\n",
    "# May add a counter to relevant pairs cached to discard pairs that are irrelevant for multiple rounds\n",
    "# for pair qualification\n",
    "\n",
    "class PoolOfQueries():\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        client: Groq,\n",
    "        embedding_function: Callable[[str], Any],\n",
    "        rerank_function: Callable[[List[Any], str], Any], # Takes in a query and the chunks and returns the ranks\n",
    "        retrieve_function: Callable[[str], Any],\n",
    "        top_k_retrieve: int = 10,\n",
    "        top_k_rerank: int = 3,\n",
    "        max_length_per_split: int = 0,\n",
    "        chunks: List[dict] = None,\n",
    "        chunks_cached: List[dict] = None,\n",
    "        unanswerable: List[dict] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.client = client\n",
    "        self.embedder = embedding_function\n",
    "        self.reranker = rerank_function\n",
    "        self.retriever = retrieve_function\n",
    "        self.chunks = chunks or []\n",
    "        self.chunks_cached = chunks_cached or []\n",
    "        self.unanswerable = unanswerable or []\n",
    "        self.max_length_per_split = max_length_per_split\n",
    "        self.top_k_retrieve = top_k_retrieve\n",
    "        self.top_k_rerank = top_k_rerank\n",
    "        \n",
    "    def update(self, messages, collection: Collection, search_params) -> None:\n",
    "        # Query Classification\n",
    "        msg = [classify_sysprompt] + messages\n",
    "        response: bool = cast(bool, get_structured_response(messages, self.client, BooleanModel).response)\n",
    "        if not response:\n",
    "            return\n",
    "        \n",
    "        # Query-context pairs qualification\n",
    "        # Seperating different type of pairs for live settings\n",
    "        relevant_pairs, irrelevant_pairs = qualify_existing_pairs(self.chunks, messages, self.client, 4)\n",
    "        relevant_pairs_cached, irrelevant_pairs_cached = qualify_existing_pairs(self.chunks_cached, messages, self.client, 4)\n",
    "        unanswerable_queries, _ = qualify_existing_pairs(self.unanswerable, messages, self.client, 4)\n",
    "        \n",
    "        #Updating each bucket\n",
    "        self.chunks = relevant_pairs + relevant_pairs_cached\n",
    "        self.chunks_cached = irrelevant_pairs + irrelevant_pairs_cached\n",
    "        self.unanswerable = unanswerable_queries\n",
    "        \n",
    "        # Generation of new queries\n",
    "        msg = [new_query_sysprompt] + messages + [new_query_prompt(self.chunks + self.hypothetical)]\n",
    "        new_queries: List[str] = get_structured_response(msg, self.client, ListStrModel).response\n",
    "        \n",
    "        # Generate HyDE\n",
    "        query_hyde: List[dict] = [get_HyDE(x, self.client) for x in new_queries]\n",
    "        \n",
    "        # Retrieve \n",
    "        retrieve_queries: List[str] = self._retrieve_queries(new_queries, query_hyde)\n",
    "        retrieve_embeddings = [self.embedder(query) for query in retrieve_queries]\n",
    "        retrieved_unranked: List[List[str]] = [self.retriever(embedded_query) for embedded_query in retrieve_embeddings]\n",
    "        \n",
    "        #Rerank\n",
    "        retrieved_ranks = [self.reranker(query, chunks) for query, chunks in zip(retrieve_queries, retrieved_unranked)]\n",
    "        retrieved_ranked = [[retrieved_unranked[i] for i in rank[:self.top_k_rerank]] for chunks, rank in zip(retrieved_unranked, retrieved_ranks)]\n",
    "        new_pairs = [self._pair_factory(query, context) for query, context in zip(retrieve_queries, retrieved_ranked)]\n",
    "        \n",
    "        # TODO: Qualify Generated\n",
    "        # Check if retrieved chunks is relevant, if not, consider the generated answer\n",
    "        # If the model thinks it can't answer it, then place the query alone in the Unanswerable bucket\n",
    "        new_pair_qualify_bool = [self._qualify_generated_pairs(pair) for pair in new_pairs]\n",
    "        new_pairs_qualified = [pair for pair, qual in zip(new_pairs, new_pair_qualify_bool) if qual]\n",
    "        new_unanswerables = [pair[\"query\"] for pair, qual in zip(new_pairs, new_pair_qualify_bool) if not qual]\n",
    "        \n",
    "        # Updating buckets again\n",
    "        self.chunks.extend(new_pairs_qualified)\n",
    "        self.unanswerable.extend(new_unanswerables)\n",
    "        \n",
    "        return\n",
    "        \n",
    "    def _qualify_generated_pairs(self, pair):\n",
    "        msg = [qualify_generated_sysprompt] + [qualify_retrieved_prompt(pair[\"query\"], pair[\"context\"])]\n",
    "        qual: bool = get_structured_response(msg, self.client, BooleanModel)\n",
    "        return qual\n",
    "        \n",
    "    # Could use some prompt engineering  here\n",
    "    def current_context(self):\n",
    "        msg = {\n",
    "            \"role\": \"system\", # For \"role\" should I use system or should I use \"Query\"\n",
    "            \"content\": f\"Current retrieved pairs: {self.chunks}, current queries that are not answerable: {self.unanswerable}\"\n",
    "        }\n",
    "        return msg\n",
    "        \n",
    "    @staticmethod\n",
    "    def _retrieve_queries(queries, hydes) -> List[str]:\n",
    "        msg_list = []\n",
    "        for query, hyde in zip(queries, hydes):\n",
    "            msg = f\"Query: {query}\"\n",
    "            if hyde.generate:\n",
    "                msg = msg + f\"Hypothetical Answer: {hydes.response}\"\n",
    "            msg_list.append(msg)\n",
    "        return msg_list\n",
    "        \n",
    "    @staticmethod\n",
    "    def _pair_factory(query: str, context: List[str]):\n",
    "        pair = {\n",
    "            \"query\": query,\n",
    "            \"context\": context\n",
    "        }\n",
    "        return pair\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatUI\n",
    "\n",
    "This is for demonstration only (a web client will be used in production)\n",
    "\n",
    "TODO: Replace the dummy_inference() function with our own inference function after that is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import random\n",
    "\n",
    "# Dummy function remains the same\n",
    "def dummy_inference(message):\n",
    "    responses = [\n",
    "        \"This is a dummy response.\",\n",
    "        \"I'm just pretending to be smart.\",\n",
    "        \"Here's some generated text for testing purposes.\",\n",
    "        \"Imagine this is a well-thought-out answer.\",\n",
    "    ]\n",
    "    return random.choice(responses)\n",
    "\n",
    "class DemoChatUI:\n",
    "    def __init__(self):\n",
    "        self.chat_history = []\n",
    "        self.output = widgets.Output()\n",
    "        self.text_input = widgets.Text(placeholder='Type your message here...')\n",
    "        self.send_button = widgets.Button(description='Send')\n",
    "        self.role_checkbox = widgets.Checkbox(description='Show roles', value=False)\n",
    "        self.reset_button = widgets.Button(description='Reset Chat')\n",
    "        \n",
    "        self.send_button.on_click(self.on_send)\n",
    "        self.text_input.on_submit(self.on_send)\n",
    "        self.role_checkbox.observe(self.update_chat_display, names='value')\n",
    "        self.reset_button.on_click(self.reset_chat)\n",
    "        \n",
    "        input_box = widgets.HBox([self.text_input, self.send_button])\n",
    "        input_box.layout.display = 'flex'\n",
    "        self.text_input.layout.flex = '1'\n",
    "        \n",
    "        bottom_box = widgets.HBox([self.reset_button, self.role_checkbox])\n",
    "        bottom_box.layout.display = 'flex'\n",
    "        bottom_box.layout.justify_content = 'space-between'\n",
    "        \n",
    "        self.chat_box = widgets.VBox([self.output, input_box, bottom_box])\n",
    "        self.main_output = widgets.Output()\n",
    "        \n",
    "        with self.main_output:\n",
    "            display(self.chat_box)\n",
    "        \n",
    "        display(self.main_output)\n",
    "        \n",
    "    def on_send(self, _):\n",
    "        user_message = self.text_input.value\n",
    "        if user_message.strip():\n",
    "            self.add_message(\"user\", user_message)\n",
    "            self.text_input.value = ''\n",
    "            \n",
    "            assistant_response = dummy_inference(user_message)\n",
    "            self.add_message(\"assistant\", assistant_response)\n",
    "            \n",
    "    def add_message(self, role, content):\n",
    "        self.chat_history.append({\"role\": role, \"content\": content})\n",
    "        self.update_chat_display()\n",
    "        \n",
    "    def update_chat_display(self, _=None):\n",
    "        self.output.clear_output()\n",
    "        with self.output:\n",
    "            for message in self.chat_history:\n",
    "                role = message['role']\n",
    "                content = message['content']\n",
    "                \n",
    "                if role == 'user':\n",
    "                    align = 'right'\n",
    "                    color = '#DCF8C6'\n",
    "                elif role == 'assistant':\n",
    "                    align = 'left'\n",
    "                    color = '#E5E5EA'\n",
    "                else:\n",
    "                    align = 'left'\n",
    "                    color = '#F3E5F5'\n",
    "                \n",
    "                role_display = f\"<small>{role}: </small>\" if self.role_checkbox.value else \"\"\n",
    "                \n",
    "                display(HTML(f\"\"\"\n",
    "                    <div style=\"text-align: {align};\">\n",
    "                        <div style=\"display: inline-block; background-color: {color}; padding: 5px 10px; border-radius: 10px; max-width: 70%;\">\n",
    "                            {role_display}{content}\n",
    "                        </div>\n",
    "                    </div>\n",
    "                \"\"\"))\n",
    "\n",
    "    def reset_chat(self, _):\n",
    "        self.chat_history = []\n",
    "        self.text_input.value = ''\n",
    "        self.role_checkbox.value = False\n",
    "        self.update_chat_display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We retrieve more chunks then we acturally provide the LLM\n",
    "top_k_retrieve = 10\n",
    "top_k_reranked = 3\n",
    "\n",
    "# Models to use\n",
    "embedding_model_name = \"dunzhang/stella_en_1.5B_v5\"\n",
    "reranker_model_name = \"castorini/monot5-base-msmarco-10k\"\n",
    "\n",
    "# Search Parameters for Milvus\n",
    "milvus_search_params = {\n",
    "    \"metric_type\": \"COSINE\",\n",
    "    'params': {\n",
    "        'nprobe': top_k_retrieve,\n",
    "        'level': 2,\n",
    "    }\n",
    "}\n",
    "\n",
    "# arguement for Groq\n",
    "groq_args = {\n",
    "    \"model\": \"llama-3.1-8b-instant\",\n",
    "    \"max_tokens\": 8192,\n",
    "    \"temperature\": 0.2, # To tune\n",
    "}\n",
    "\n",
    "groq_args_structured = {\n",
    "    \"model\": \"llama-3.1-8b-instant\",\n",
    "    \"max_tokens\": 8192,\n",
    "    \"temperature\": 0.2, # To tune\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "from pymilvus import connections, Collection\n",
    "from functools import partial\n",
    "\n",
    "# File Path\n",
    "documents_path = os.path.join(os.getcwd(), \"documents\") # For demo purposes can only assume \"documents\" is in root directory\n",
    "\n",
    "# Setting up VectorDB (milvus)\n",
    "# The collection's schema \"schema\" is defined in the DB portion of the field\n",
    "connections.connect(host='localhost', port='19530')\n",
    "client = Groq(api_key = GROQ_API_KEY)\n",
    "collection = Collection(name=\"documents\", schema=schema)\n",
    "\n",
    "# Preparing embedding function\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "embedding_model = AutoModel.from_pretrained(embedding_model_name).to('cuda')\n",
    "embedding_model.eval()\n",
    "embedding_function = partial(get_embeddings, \n",
    "                             embedding_tokenizer = embedding_tokenizer, \n",
    "                             embedding_model = embedding_model)\n",
    "\n",
    "# Preparing reranker function (This is wrong, will work on it later)\n",
    "reranker_tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)\n",
    "reranker_model = AutoModelForSeq2SeqLM.from_pretrained(reranker_model_name).to('cuda')\n",
    "reranker_model.eval()\n",
    "reranker_function = partial(get_ranks, \n",
    "                            reranker_tokenizer = reranker_tokenizer, \n",
    "                            reranker_model = reranker_model)\n",
    "\n",
    "# Retriever function\n",
    "retrieve_function = partial(retrieve, \n",
    "                            top_k_retrieve = top_k_retrieve, \n",
    "                            collection = collection, \n",
    "                            search_params = milvus_search_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Workflow\n",
    "documents = read_directory(documents_path, recursive = True)\n",
    "store_and_embed_documents(documents, collection, embedding_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ui = DemoChatUI()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
