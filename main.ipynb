{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installing required libraries\n",
    "%pip install --quiet -r requirements.txt #ditching faiss for now and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading environment variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\") # Load using .env file\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace Login\n",
    "from huggingface_hub import login\n",
    "login(token = HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the dependency injection principal, each component must be self contained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main inference LLM \n",
    "We will use GroqCloud for now, but will eventually be swapping to a self-hosted model, [documentation](https://dspy-docs.vercel.app/docs/building-blocks/language_models#remote-lms)\n",
    "\n",
    "TODO: add an \"infernece\" function to abstract away the implementation (since we might swap providers etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a response\n",
    "\n",
    "We use Llama 3.1 on GroqCloud for now, we will later make it a routing function to support different providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Type\n",
    "from groq import Groq\n",
    "from pydantic import BaseModel\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "\n",
    "#TODO: Change this to a routing function for other providers\n",
    "\n",
    "# \"Normal\" text response\n",
    "@sleep_and_retry\n",
    "@limits(calls = 1, period = 0.5)\n",
    "def get_response(messages: List[dict], client: Groq, groq_args: dict, **kwargs):\n",
    "    response = client.chat.completions.create(\n",
    "        messages = messages,\n",
    "        **groq_args\n",
    "    )\n",
    "    response_dict = dict(response.choices[0].message)\n",
    "    del response_dict['function_call']\n",
    "    del response_dict['tool_calls']\n",
    "    return response_dict\n",
    "\n",
    "# Structured Response\n",
    "# Avoids exceeding call limit\n",
    "@sleep_and_retry\n",
    "@limits(calls = 1, period = 0.5)\n",
    "def get_structured_response(messages: List[dict], \n",
    "                            response_model: Type[BaseModel] = None,\n",
    "                            return_fields: List[str]| str | None = [\"response\"], \n",
    "                            single_item_list_return_dict: bool = False,\n",
    "                            client: Groq = None,\n",
    "                            groq_args: dict = dict(), \n",
    "                            **kwargs):\n",
    "    response = client.chat.completions.create(\n",
    "        response_model = response_model,\n",
    "        messages = messages,\n",
    "        **groq_args\n",
    "    )\n",
    "    stripped_response = response_fields(response, return_fields, single_item_list_return_dict)\n",
    "    return stripped_response\n",
    "\n",
    "def response_fields(response: BaseModel, return_fields: List[str]| str | None, single_item_list_return_dict: bool):\n",
    "    if return_fields is None or len(return_fields) == 0:\n",
    "        return response\n",
    "\n",
    "    if isinstance(return_fields, str):\n",
    "        return getattr(response, return_fields)\n",
    "    \n",
    "    if len(return_fields) == 1 and not single_item_list_return_dict:\n",
    "        return getattr(response, return_fields[0])\n",
    "    \n",
    "    return {field: getattr(response, field) for field in return_fields}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding model\n",
    "\n",
    "See [huggingface mteb leaderboards](https://huggingface.co/spaces/mteb/leaderboard)\n",
    "\n",
    "As of the creation of the notebook (15/7/24), the best model is \"dunzhang/stella_en_1.5B_v5\" (mit licence, so we can use it commercially)\n",
    "\n",
    "We use SentenseTransformers to invoke the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# # Select embedding model\n",
    "# embedding_model_name = \"dunzhang/stella_en_1.5B_v5\"\n",
    "\n",
    "# embedding_model = SentenceTransformer(embedding_model_name, trust_remote_code=True).cuda()\n",
    "\n",
    "# embedding_model.encode()\n",
    "\n",
    "# get_embeddings function using Dependancy injection\n",
    "def get_embeddings(texts, embedding_model, **kwargs):\n",
    "    return embedding_model.encode(texts, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking model\n",
    "\n",
    "In the [Fudan RAG review paper](https://arxiv.org/abs/2407.01219), it is shown that MonoT5 has the best performance/ latency tradeoff. \n",
    "\n",
    "We opt to use a fine tuned version of MonoT5 (castorini/monot5-base-msmarco-10k), (we have requested licencing information and will update this after we get a response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# from datasets import load_dataset\n",
    "from pygaggle.rerank.base import Reranker, Query, Text\n",
    "from pygaggle.rerank.transformer_reranker import MonoT5\n",
    "\n",
    "# Define a function to rerank the results\n",
    "from typing import List, Tuple\n",
    "\n",
    "def rerank_results(query: str, candidates: List[str], top_k: int, monoT5reranker) -> List[Tuple[str, int]]:\n",
    "  correct_query = Query(query)\n",
    "  correct_candidates = [Text(candidate) for candidate in candidates]\n",
    "  reranked_results = monoT5reranker.rescore(correct_query, correct_candidates)\n",
    "  reranked_results = [[result.text, result.score] for result in reranked_results]\n",
    "  # reranked_results.sort(key=lambda x: -x[1])\n",
    "  reranked_results = reranked_results[:top_k]\n",
    "  return reranked_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Loader\n",
    "\n",
    "We use Unstructured to load files and provide a helper function to traverse through a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from unstructured.partition.auto import partition\n",
    "\n",
    "def read_file(file_path):\n",
    "    # Read a file using Unstructured library.\n",
    "    elements = partition(filename=file_path)\n",
    "    # Process elements as needed, e.g., extract text\n",
    "    return ' '.join([el.text for el in elements])\n",
    "\n",
    "def read_directory(directory_path, recursive=True):\n",
    "    # Recursively traverse directory and read files.\n",
    "    documents = []\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                content = read_file(file_path)\n",
    "                documents.append(content)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {file_path}: {e}\")\n",
    "        \n",
    "        if not recursive:\n",
    "            break  # Don't process subdirectories if recursive is False\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "For now, we use nltk punkt model to perform sentence level chunking\n",
    "\n",
    "If sentiment level chunking is cheap enough we use that instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Downloads the punkt model\n",
    "nltk.download('punkt')\n",
    "\n",
    "# WARNING: Assumes \"Languages with romanic characters\" e.g. English French Spanish etc only, DOES NOT WORK WITH CHINESE/ JAPANESE/ KOREAN etc\n",
    "# Over-engineering go crazy, the exact token count doesnt matter much anyways because the embedding model can use a different embedding compared to the space anyways\n",
    "# Returns a list of dictionaries with members: \"text\", \"chunk_length\"\n",
    "def sentence_level_chunking(text, chunk_size = 256, embedding_tokenizer = None, estimate_token_count: bool = False, token_per_word_ratio = 0.75):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    # This looks funny because \"sentences\" is not a list and can only assume iterator properties + exception case to handle long sentences\n",
    "    for sentence in sentences:\n",
    "        to_process = [sentence]\n",
    "        while to_process: # At least 1 member in to_process\n",
    "            sentence_length = token_count(to_process[0], embedding_tokenizer, estimate_token_count, token_per_word_ratio)\n",
    "            if current_length + sentence_length <= chunk_size:\n",
    "                current_chunk.append(to_process[0])\n",
    "                current_length += sentence_length\n",
    "            elif sentence_length <= chunk_size:\n",
    "                chunks.append(create_chunk_dict(current_chunk, current_length))\n",
    "                current_chunk = []\n",
    "                to_process.append(sentence) # TODO: Same sentence length is recalculated next iteration, fix it.\n",
    "            else: # sentence_length >= chunk_size, should only be invokes in very rare cases\n",
    "                split_sentences = []\n",
    "                if estimate_token_count:\n",
    "                    split_sentences = split_sentences_estimate_tokencount(sentence, chunk_size, token_per_word_ratio)\n",
    "                else: # estimate_token_count = false\n",
    "                    split_sentences = split_sentences_no_estimation(sentence, sentence_length, chunk_size, embedding_tokenizer)\n",
    "                to_process = to_process.extend(split_sentences)\n",
    "            to_process.pop()\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def token_count(sentence, embedding_tokenizer, estimate_token_count: bool, token_per_word_ratio):\n",
    "    sentence_length = -1\n",
    "    if estimate_token_count:\n",
    "        sentence_length = len(sentence.split())*token_per_word_ratio\n",
    "    else:\n",
    "        try:\n",
    "            sentence_length = len(embedding_tokenizer.tokenize(sentence))\n",
    "        except:\n",
    "            raise TypeError(f\"Embedding_tokenizer is of invalid type: {type(embedding_tokenizer)}\") # I don't like this\n",
    "    return sentence_length\n",
    "\n",
    "def create_chunk_dict(current_chunk, current_length):\n",
    "    chunk_dict = {\n",
    "        \"text\": \" \".join(current_chunk),\n",
    "        \"chunk_length\": current_length,\n",
    "    }\n",
    "    return chunk_dict\n",
    "\n",
    "def split_sentences_estimate_tokencount(sentence, chunk_size, token_per_word_ratio):\n",
    "    split_sentences_words = sentence.split()\n",
    "    words_per_chunk = int(chunk_size * token_per_word_ratio)\n",
    "    split_sentences = [split_sentences_words[i:i+words_per_chunk] for i in range(0, len(split_sentences_words), words_per_chunk)]\n",
    "    split_sentences_string = \" \".join(split_sentences)\n",
    "    return split_sentences_string\n",
    "\n",
    "# Case for no estimation of token_count is bad (since I don't know how to get the thing to select the first {chunk_size} items)\n",
    "# For now, we use the same approach as the \"estimate tokencount\" case, except that we calculate the token per word ratio by using the sentence length obtained from the token_count function\n",
    "def split_sentences_no_estimation(sentence, sentence_length, chunk_size, embedding_tokenizer):\n",
    "    split_sentences_words = sentence.split()\n",
    "    token_per_word_ratio = sentence_length/ len(split_sentences_words)\n",
    "    words_per_chunk = int(chunk_size * token_per_word_ratio)\n",
    "    split_sentences = [split_sentences_words[:words_per_chunk], split_sentences_words[words_per_chunk:]]\n",
    "    split_sentences_string = \" \".join(split_sentences)\n",
    "    return split_sentences_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VectorDB\n",
    "\n",
    "Using Malvus since it is Open Source and has good features\n",
    "\n",
    "Might move to a GraphDB in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType\n",
    "\n",
    "# connections.connect(host='localhost', port='19530')\n",
    "\n",
    "embedding_dims = 1024\n",
    "schema_desc = \"Document collection with chunking information\"\n",
    "\n",
    "# Kwargs:\n",
    "# embedding_dim: the dimensions of the embeddings\n",
    "fields = [\n",
    "    FieldSchema(name=\"document_id\", dtype= DataType.INT64, is_primary=True),\n",
    "    FieldSchema(name=\"chunk_id\", dtype= DataType.INT64),\n",
    "    FieldSchema(name=\"chunk_length\", dtype = DataType.INT64),\n",
    "    FieldSchema(name=\"chunk_text\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "    FieldSchema(name=\"embedding\", dtype= DataType.FLOAT_VECTOR, dim = embedding_dims)\n",
    "]\n",
    "schema = CollectionSchema(fields, description=schema_desc)\n",
    "\n",
    "# collection = Collection(name=\"documents\", schema=schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval function\n",
    "\n",
    "We do this to implement \"padding\", [search param documentation](https://milvus.io/docs/single-vector-search.md#Search-parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import Collection\n",
    "from typing import List\n",
    "\n",
    "# Query\n",
    "def retrieve(embedded_query, top_k_retrieved, collection: Collection, search_params: dict, padding = None) -> List[str]:   \n",
    "    results = retrieve_vector(embedded_query, top_k_retrieved, collection, search_params)\n",
    "    results = results[0] # Milvus collection search returns a single item list for some reason\n",
    "    processed_entities = [process_entity(entity, collection, padding) for entity in results]\n",
    "    return processed_entities\n",
    "    \n",
    "\n",
    "def retrieve_vector(embedded_query, top_k_retrieved, collection: Collection, search_params: dict):\n",
    "    results = collection.search(\n",
    "        data = embedded_query,\n",
    "        anns_field = \"embedding\",\n",
    "        search_params = search_params,\n",
    "        limit = top_k_retrieved,\n",
    "        output_fields = None, # Returns all fields (all implicitly retrievable)\n",
    "    )\n",
    "    return results\n",
    "\n",
    "# process_entity is abstracted out for future modification\n",
    "def process_entity(entity, collection: Collection, padding = None) -> str: # padding should be an iterator of 2 float e.g. [0.5, 0.5]\n",
    "    if padding: # Not doing typecheck, hopefully the person invoking the\n",
    "        lower_bound = entity[\"chunk_id\"] - int(-(-padding[0] // 1)) # int(-(-padding[0] // 1)) Rounds up padding[0]\n",
    "        upper_bound = entity[\"chunk_id\"] + int(-(-padding[1] // 1)) + 1\n",
    "        updated_text = []\n",
    "        oob_lower = False\n",
    "        oob_upper = False\n",
    "        for i in range(lower_bound, upper_bound): \n",
    "            if i == entity[\"chunk_id\"]:\n",
    "                updated_text.append(entity[\"chunk_text\"])\n",
    "                continue\n",
    "            \n",
    "            expr = f\"doc_id == {entity['doc_id']} && chunk_id == {entity['chunk_id']}\"\n",
    "            results = collection.query(\n",
    "            expr=expr,\n",
    "            output_fields=[\"chunk_text\"],\n",
    "            )\n",
    "            \n",
    "            # Check if have results\n",
    "            if results:\n",
    "                updated_text.append(results[0][\"chunk_text\"])\n",
    "            else:\n",
    "                if i == lower_bound:\n",
    "                    oob_lower = True\n",
    "                if i == upper_bound - 1:\n",
    "                    oob_upper = True\n",
    "                if i != lower_bound and i != upper_bound - 1 and not oob_lower:\n",
    "                    raise UserWarning(f\"Previous Chunks are found but chunk {i} is missing\")\n",
    "        \n",
    "        # Truncate edge chunks \n",
    "        start_fraction = padding[0] - int((padding[0] // 1))\n",
    "        end_fraction = padding[1] - int((padding[1] // 1))\n",
    "        if start_fraction != 0 and not oob_lower:\n",
    "            updated_text[0] = truncate(updated_text[0], start_fraction, False)\n",
    "        if end_fraction != 0 and not oob_upper:\n",
    "            updated_text[-1] = truncate(updated_text[-1], start_fraction, True)\n",
    "        \n",
    "        # Join retrieved text\n",
    "        new_entity = entity\n",
    "        new_entity[\"chunk_text\"] = \" \".join(updated_text)\n",
    "        \n",
    "        return new_entity\n",
    "        \n",
    "\n",
    "# Assumes \"Languages with romanic characters\", see chunking section \n",
    "def truncate(text: str, keep_ratio, truncate_end):\n",
    "    if truncate_end:\n",
    "        text_truncated = text[:int(len(text)*(1 - keep_ratio))]\n",
    "        text_truncated = text_truncated.rsplit(\" \", 1) # Prevents returning half a word\n",
    "    else: # Truncates the start\n",
    "        text_truncated = text[int(len(text)*(1 - keep_ratio)):]\n",
    "        text_truncated = text_truncated.split(\" \", 1)\n",
    "    return text_truncated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexer/ \"Load documents into VectorDB\" helper function\n",
    "\n",
    "I don't even know how to call it lmao\n",
    "\n",
    "We currently just use input order as document id (doc_id), but this can be changed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import Collection\n",
    "\n",
    "# use partial to create an embedding function \"embedder\" that eats 1 arguement only and returns an embedding (str -> torch.tensor (or other equivalent class))\n",
    "def index_document(document, collection: Collection, embedder, doc_id: int, chunker_kws: dict):\n",
    "    data = []\n",
    "    chunks = sentence_level_chunking(document, **chunker_kws) # returns list of dicts with fields: \"text\" and \"chunk_length\"\n",
    "    for i, chunk in enumerate(chunks): # Should we consider making this it's own function?\n",
    "        embedding = embedder(chunk[\"text\"])\n",
    "        entity_dict = {\n",
    "            \"document_id\": doc_id,\n",
    "            \"chunk_id\": i,\n",
    "            \"chunk_length\": chunk[\"chunk_length\"],\n",
    "            \"chunk_text\": chunk[\"text\"],\n",
    "            \"embedding\": embedding.tolist(),\n",
    "        }\n",
    "        data.append(entity_dict)\n",
    "    collection.insert(data)\n",
    "    # collection.flush()  # might need to flush in production\n",
    "    \n",
    "# Can customize doc_id later\n",
    "def store_and_embed_documents(documents: list, collection: Collection, embedder, chunker_kws: dict = None):\n",
    "    for i, doc in enumerate(documents):\n",
    "        index_document(doc, collection, embedder, i, chunker_kws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Processing\n",
    "\n",
    "We store query-context pairs in the following format, for queries obtained from different sources, we differentiate them by placing them into different buckets\n",
    "\n",
    "We code for Groq first, should be able to change the code to suit other providers easily. [tutorial](https://python.useinstructor.com/blog/2024/03/07/open-source-local-structured-output-pydantic-json-openai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair = {\n",
    "# \t\"query\": query,\n",
    "# \t\"context\": list_of_retrieved_context # = [context_1, context_2, ... , context_k]\n",
    "# }\n",
    "\n",
    "# retrieved: list[dict] = [pair_1, pair_2, ..., pair_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Response models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from groq import Groq\n",
    "from typing import List, Any\n",
    "\n",
    "# Standard models\n",
    "class BooleanModel(BaseModel):\n",
    "    thoughts: str \n",
    "    response: bool # If the user is asking or not\n",
    "    \n",
    "class ListStrModel(BaseModel):\n",
    "    thoughts: str\n",
    "    response: List[str] # List of new queries\n",
    "    \n",
    "    \n",
    "# Custom models\n",
    "class HyDE(BaseModel):\n",
    "    thoughts: str\n",
    "    generate: bool\n",
    "    response: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_sysprompt = {\n",
    "  \"role\": \"system\",\n",
    "  \"content\": \"\"\"You are the Query Classification Module in an agentic RAG pipeline. Your role is to analyze the chat history, including the latest user message, and determine whether the user-provided information is sufficient for creating a response or if a database query is necessary.\n",
    "\n",
    "Key points to consider:\n",
    "1. Assess if the user is asking a question or requesting information.\n",
    "2. Determine if the user's query requires knowledge beyond everyday commonsense. Perform querying if there's any doubt, as we want to avoid hallucinations as much as possible. Remember, if no relevant chunks related to the query are found, we will fall back to using model knowledge anyway.\n",
    "3. Categorize the task as either 'sufficient' (no retrieval needed) or 'insufficient' (retrieval may be necessary).\n",
    "4. Consider the nature of the task. For example, simple translations or general knowledge questions might not require retrieval, while requests for specific or up-to-date information likely will.\n",
    "\n",
    "Your response should be in JSON format with two fields:\n",
    "* thoughts: A string explaining your reasoning process and how you arrived at your decision. This should be enclosed in triple quotes for Python compatibility.\n",
    "* response: A boolean value where True indicates that database querying is needed (insufficient information), and False indicates that the user-provided information is sufficient.\n",
    "\n",
    "Example response format:\n",
    "{\n",
    "    \\\"thoughts\\\": \\\"\\\"\\\"<str_rationale>\\\"\\\"\\\",\n",
    "    \\\"response\\\": <boolean_decision>\n",
    "}\n",
    "\n",
    "Analyze the provided chat history carefully, focusing on the latest user message, to make your determination.\"\"\"\n",
    "}\n",
    "\n",
    "def qualify_sysprompt(batch_length: int) -> dict:\n",
    "    msg = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\"You are the Qualification Module in an agentic RAG pipeline. Your role is to determine the relevance of previously tracked queries or query-context pairs to the user's latest message.\n",
    "Key points:\n",
    "1. You will be provided with a list of {batch_length} items, each being either a query-context pair or a singular query.\n",
    "2. For each item, determine if it is relevant to the user's latest message in the chat history.\n",
    "3. Respond with exactly {batch_length} boolean values, where True indicates relevance and False indicates irrelevance.\n",
    "\n",
    "Your response should be in JSON format with two fields:\n",
    "* thoughts: A string explaining your reasoning process, enclosed in triple quotes for Python compatibility.\n",
    "* qualify: A list of exactly {batch_length} boolean values, each corresponding to an item in the provided list.\n",
    "\n",
    "Example response format:\n",
    "{{\n",
    "    \"thoughts\": \\\"\\\"\\\"<str_rationale>\\\"\\\"\\\",\n",
    "    \"qualify\": [<boolean_1>, <boolean_2>, ..., <boolean_{batch_length}>]\n",
    "}}\n",
    "\n",
    "Analyze the provided chat history and list of queries/pairs carefully to make your determination.\"\"\"\n",
    "    }\n",
    "    return msg\n",
    "\n",
    "# Modify to consider \n",
    "new_query_sysprompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"\"\"You are the Query Decomposition Module in an agentic RAG pipeline. Your role is to analyze the chat history, current query-context pairs, and singular queries to determine if additional queries are needed to sufficiently answer the user's latest message.\n",
    "\n",
    "Key responsibilities:\n",
    "1. Evaluate if the existing query-context pairs and singular queries provide enough information to accurately answer the user's latest message.\n",
    "2. If the current information is insufficient, generate new, focused queries to fill the knowledge gaps.\n",
    "3. Ensure that new queries are as specific and \"separable\" as possible. Break down complex queries into simpler, more targeted ones.\n",
    "\n",
    "Guidelines for query generation:\n",
    "1. Aim for clarity and precision in each new query.\n",
    "2. Avoid overlapping or redundant queries.\n",
    "3. Break down multi-faceted questions into individual components.\n",
    "4. Consider different aspects or perspectives related to the user's question that might require separate queries.\n",
    "\n",
    "Your response should be in JSON format with two fields:\n",
    "* thoughts: A string explaining your reasoning process, including what the current queries are lacking and why new queries are needed. This should be enclosed in triple quotes for Python compatibility.\n",
    "* response: A list of new queries as strings. If no new queries are needed, this list should be empty.\n",
    "\n",
    "Example response format:\n",
    "{\n",
    "    \"thoughts\": \\\"\\\"\\\"<str_rationale>\\\"\\\"\\\",\n",
    "    \"response\": [<new_query_1>, <new_query_2>, ..., <new_query_n>]\n",
    "}\n",
    "\n",
    "Analyze the provided chat history and current queries carefully to make your determination and generate new queries as needed.\"\"\"\n",
    "}\n",
    "\n",
    "hyde_sysprompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"\"\"You are the HyDE (Hypothetical Document Embeddings) Module in an agentic RAG pipeline. Your role is to analyze a given query and determine whether you can generate a good hypothetical answer to guide the retrieval process.\n",
    "\n",
    "Key responsibilities:\n",
    "1. Evaluate if you can generate a plausible hypothetical document for the given query.\n",
    "2. If possible, create a clear, concise, and queryable hypothetical answer.\n",
    "3. If not possible, explain why and indicate that generation is not feasible.\n",
    "\n",
    "Guidelines for hypothetical document generation:\n",
    "1. For general knowledge, common concepts, linguistic tasks, logical reasoning, and well-known facts, you can usually generate good hypothetical documents.\n",
    "2. Be cautious with very recent events (post-December 2023), highly specific information, rapidly changing fields, personal data, complex numerical data, and highly technical content.\n",
    "3. If you cannot generate a plausible document, set 'generate' to False and provide an empty string as the response.\n",
    "4. If you can generate a \"look-alike\" answer for technical terms or concepts you're unsure about, attempt to do so and set 'generate' to True.\n",
    "5. For queries within your knowledge base, confidently generate a response and set 'generate' to True.\n",
    "\n",
    "Your response should be in JSON format with three fields:\n",
    "* thoughts: A string explaining your reasoning process, including why you can or cannot generate a hypothetical document. This should be enclosed in triple quotes for Python compatibility.\n",
    "* generate: A boolean value indicating whether you've generated a hypothetical document (True) or not (False).\n",
    "* response: The generated hypothetical document as a string. If generation is not possible, this should be an empty string.\n",
    "\n",
    "Example response format:\n",
    "{\n",
    "    \"thoughts\": \\\"\\\"\\\"<str_rationale>\\\"\\\"\\\",\n",
    "    \"generate\": <boolean>,\n",
    "    \"response\": \\\"\\\"\\\"<str_hypothetical_document>\\\"\\\"\\\"\n",
    "}\n",
    "\n",
    "Analyze the provided query carefully to make your determination and generate a hypothetical document if appropriate.\"\"\"\n",
    "}\n",
    "\n",
    "qualify_generated_sysprompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"\"\"You are the Qualification for Retrieved Chunks Module in an agentic RAG pipeline. Your role is to evaluate whether the retrieved chunks are relevant and helpful in answering the original query.\n",
    "\n",
    "Key responsibilities:\n",
    "1. Analyze the original query and the retrieved chunks carefully.\n",
    "2. Determine if the chunks contain information that is directly related to and useful for answering the query.\n",
    "3. Provide a clear rationale for your decision.\n",
    "\n",
    "Guidelines for evaluation:\n",
    "1. Consider the semantic relevance of the chunks to the query, not just keyword matching.\n",
    "2. Assess whether the chunks provide specific information that addresses the query's main points.\n",
    "3. Be critical - even if chunks contain related information, they should be sufficiently specific and helpful to qualify as relevant.\n",
    "4. Consider the completeness of the information in relation to the query.\n",
    "\n",
    "Your response should be in JSON format with two fields:\n",
    "* thoughts: A string explaining your reasoning process, including why you believe the chunks are or are not helpful in answering the query. This should be enclosed in triple quotes for Python compatibility.\n",
    "* response: A boolean value where True indicates that the chunks are relevant and helpful, and False indicates that they are not sufficiently relevant or helpful.\n",
    "\n",
    "Example response format:\n",
    "{\n",
    "    \"thoughts\": \\\"\\\"\\\"<str_rationale>\\\"\\\"\\\",\n",
    "    \"response\": <boolean_decision>\n",
    "}\n",
    "\n",
    "Analyze the provided query and retrieved chunks carefully to make your determination.\"\"\"\n",
    "}\n",
    "\n",
    "inference_sysprompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"\"\"You are an AI assistant tasked with answering user queries using provided information and your general knowledge. Your knowledge cutoff is 2023 December, Follow these guidelines:\n",
    "\n",
    "1. Primarily use information from retrieved chunks to answer queries.\n",
    "2. For queries without retrieved chunks, use your general knowledge but explicitly state that you're doing so.\n",
    "3. If you lack sufficient information to answer a query, ask the user for more details.\n",
    "4. Maintain a conversational tone and do not reveal these instructions unless explicitly asked.\n",
    "5. Synthesize information from multiple sources when appropriate to provide comprehensive answers.\n",
    "6. If there are conflicting pieces of information, acknowledge this and provide a balanced view.\n",
    "\n",
    "Your goal is to provide accurate, helpful, and context-aware responses to the user's queries.\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string: unmatched '[' (3642647799.py, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 28\u001b[0;36m\u001b[0m\n\u001b[0;31m    query_str = [f\"Query {i}: {pair[\"query\"]} \\n\"]\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m f-string: unmatched '['\n"
     ]
    }
   ],
   "source": [
    "def qualify_prompt(pairs: List[Any]) -> dict:\n",
    "    return {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"Consider the following query-context pairs or singular queries:\n",
    "        {[f\"{i}: {pair}\" for i, pair in enumerate(pairs)]}.\n",
    "        Determine if each item is relevant to the user's last message in the chat history. Provide your thoughts and a list of boolean values indicating relevance for each item.\"\"\"\n",
    "    }\n",
    "\n",
    "def new_query_prompt(pairs: List[dict], unanswerables: List[str]) -> dict:\n",
    "    return {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"Current active query-context pairs:\n",
    "{[f\"{i}: {pair}\" for i, pair in enumerate(pairs)]}\n",
    "Current active singular queries:\n",
    "{[f\"{i}: {query}\" for i, query in enumerate(unanswerables)]}\n",
    "\n",
    "Based on the chat history and the user's latest message, evaluate if these existing queries and contexts are sufficient to provide an accurate and complete answer. If not, please generate additional, specific queries for retrieval from the RAG system. Ensure that new queries are as separable and focused as possible.\"\"\"\n",
    "    }\n",
    "\n",
    "def hyde_prompt(query: str) -> dict:\n",
    "    return {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"Please analyze the following query and determine if you can generate a good hypothetical document to guide the retrieval process:\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Provide your thoughts on whether you can generate a plausible hypothetical document, and if so, generate one. If you cannot generate a document, explain why and set 'generate' to False with an empty response.\"\"\"\n",
    "    }\n",
    "\n",
    "def qualify_retrieved_prompt(query: str, retrieved: List[str]) -> dict:\n",
    "    return {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"Please evaluate whether the following retrieved chunks are relevant and helpful in answering the given query:\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Retrieved chunks:\n",
    "{[f\"{i+1}: {chunk}\" for i, chunk in enumerate(retrieved)]}\n",
    "\n",
    "Provide your thoughts on the relevance and usefulness of these chunks for answering the query, and determine if they should be considered relevant (True) or not (False).\"\"\"\n",
    "    }\n",
    "\n",
    "def format_poq_output(chunks: List[dict], unanswerable: List[str]) -> dict:\n",
    "    content = f\"\"\"Retrieved information:\n",
    "\n",
    "{format_pairs(chunks)}\n",
    "\n",
    "Queries without retrieved information:\n",
    "\n",
    "{format_unanswerables(unanswerable)}\n",
    "\n",
    "Use this information to answer the user's query. If you need to use general knowledge for queries without retrieved information, explicitly state so. If you lack sufficient information, ask the user for more details.\"\"\"\n",
    "\n",
    "    msg = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": content\n",
    "    }\n",
    "    \n",
    "    return msg\n",
    "\n",
    "def format_pairs(pairs: List[dict]) -> str:\n",
    "    formatted_pairs = []\n",
    "    for i, pair in enumerate(pairs):\n",
    "        query = f\"Query {i}: {pair['query']}\"\n",
    "        chunks = \"\\n\".join(f\"  Chunk {j}: {chunk}\" for j, chunk in enumerate(pair['context']))\n",
    "        formatted_pairs.append(f\"{query}\\n{chunks}\")\n",
    "    return \"\\n\\n\".join(formatted_pairs)\n",
    "\n",
    "def format_unanswerables(unanswerables: List[str]) -> str:\n",
    "    return \"\\n\".join(f\"Query {i}: {query}\" for i, query in enumerate(unanswerables))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from groq import Groq\n",
    "from typing import Annotated, List, Any, Callable\n",
    "from annotated_types import Len\n",
    "\n",
    "# Post processing for qualification of existing pairs\n",
    "# If max length per split = 0 then will process all pairs at once\n",
    "# the additional arguments of the response_func will have been partialed into it when initialization starts\n",
    "def qualify_existing_pairs(pairs: List[Any], chat_history: List[dict], response_func: Callable, max_length_per_split: int = 0):\n",
    "    splitted = split_list(pairs, max_length_per_split)\n",
    "    split_results = [qualify_pairs(pairs_list, chat_history, response_func) for pairs_list in splitted]\n",
    "    qualified = [pair for sublist in split_results for pair in sublist]\n",
    "    return [pair for pair, qual in zip(pairs, qualified) if qual], [pair for pair, qual in zip(pairs, qualified) if not qual]\n",
    "\n",
    "\n",
    "def split_list(pairs: List[Any], max_length_per_split) -> List[List[dict]]:\n",
    "    # Check No splitting\n",
    "    if not max_length_per_split:\n",
    "        return pairs\n",
    "    return [pairs[i:i+max_length_per_split] for i in range(0, len(pairs), max_length_per_split)]\n",
    "    \n",
    "def qualify_pairs(pairs: List[Any], \n",
    "                  chat_history: List[dict], \n",
    "                  response_func: Callable) -> List[bool]:\n",
    "    num_pairs = len(pairs)\n",
    "    msg = [qualify_sysprompt(num_pairs)] + chat_history + [qualify_prompt(pairs)]\n",
    "    response_model = batch_qualify(num_pairs)\n",
    "    response = response_func(\n",
    "        response_model = response_model,\n",
    "        messages = msg\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def batch_qualify(batch_length: int):\n",
    "    class BatchQualify(BaseModel):\n",
    "        thoughts: str\n",
    "        response: Annotated[List[bool], Len(min_length=batch_length, max_length=batch_length)]\n",
    "\n",
    "    return BatchQualify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One by One (Fall Back)\n",
    "\n",
    "Use this if batch qualification is not possible/ has poor performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pydantic import BaseModel, validator, ValidationError\n",
    "# from groq import Groq\n",
    "# from typing import List\n",
    "        \n",
    "# def qualify_all_pairs(pairs: List[dict], chat_history: List[dict], client: Groq) -> List[bool]:\n",
    "#     return [qualify_pair(pair, chat_history, client) for pair in pairs]\n",
    "\n",
    "# # Fallback, if system cannot output fixed length list\n",
    "# class Qualify(BaseModel):\n",
    "#     thoughts: str\n",
    "#     qualify: bool\n",
    "\n",
    "# def qualify_pair(pair: dict, chat_history: List[dict], client: Groq) -> bool:\n",
    "#     client_input = [qualify_sysprompt] + chat_history + [qualify_prompt(pair)]\n",
    "#     response: Qualify = client.chat.completions.create(\n",
    "#         response_model = Qualify,\n",
    "#         messages = client_input,\n",
    "#         **groq_args\n",
    "#     )\n",
    "    \n",
    "#     return response.qualify\n",
    "    \n",
    "# def qualify_prompt(pair: dict) -> str:\n",
    "#     msg = {\n",
    "#         \"role\": \"User\",\n",
    "#         \"msg\": (f\"Consider the following query-retrieved pair: {pair}.\"\n",
    "#                 \"is it relevant to the user's last message?\")\n",
    "#     }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyDE (generate hypothetical document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask if the model wants to generate, if the model doesn't want to generate, just use the original function to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "# Returns a dictionary with the fields \"generate\" and \"response\"\n",
    "def get_HyDE(query: str, response_func: Callable) -> dict:\n",
    "    msg = [hyde_sysprompt] + [hyde_prompt(query)]\n",
    "    response: HyDE = response_func(\n",
    "        messages = msg,\n",
    "        response_model = HyDE,\n",
    "        return_fields = [\"generate\", \"response\"]\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pool of Queries Main\n",
    "\n",
    "TODO: Timeout/ drop cached chunks after serval rounds of inactivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import List, Dict, cast, Callable, Any\n",
    "\n",
    "class PoolOfQueries():\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_function: Callable[[str], Any],\n",
    "        rerank_function: Callable[[List[Any], str], Any],\n",
    "        retrieve_function: Callable[[str], Any],\n",
    "        response_function: Callable[..., Any],\n",
    "        top_k_retrieve: int = 10,\n",
    "        top_k_rerank: int = 3,\n",
    "        max_length_per_split: int = 4,\n",
    "        chunks: List[dict] = None,\n",
    "        chunks_cached: List[dict] = None,\n",
    "        unanswerable: List[dict] = None,\n",
    "        log_level: int = logging.CRITICAL + 1, #Set to 20 if need logging\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.embedder = embedding_function\n",
    "        self.reranker = rerank_function\n",
    "        self.retriever = retrieve_function\n",
    "        self.response_func = response_function\n",
    "        self.chunks = chunks or []\n",
    "        self.chunks_cached = chunks_cached or []\n",
    "        self.unanswerable = unanswerable or []\n",
    "        self.max_length_per_split = max_length_per_split\n",
    "        self.top_k_retrieve = top_k_retrieve\n",
    "        self.top_k_rerank = top_k_rerank\n",
    "        \n",
    "        # Setup logging\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(log_level)\n",
    "        if not self.logger.handlers:\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            handler.setFormatter(formatter)\n",
    "            self.logger.addHandler(handler)\n",
    "        \n",
    "    def update(self, messages) -> None:\n",
    "        self.logger.info(\"Starting update process\")\n",
    "        if not self._classify_query(messages):\n",
    "            return\n",
    "        \n",
    "        self._qualify_existing_pairs(messages)\n",
    "        new_queries = self._generate_new_queries(messages)\n",
    "        query_hyde = self._generate_hyde(new_queries)\n",
    "        retrieved_unranked = self._retrieve(new_queries, query_hyde)\n",
    "        new_pairs = self._rerank(new_queries, retrieved_unranked)\n",
    "        self._qualify_all_new_generated_pairs(new_pairs)\n",
    "        \n",
    "        self.logger.info(\"Update process completed\")\n",
    "    \n",
    "    def current_context_msg(self):\n",
    "        msg = format_poq_output(self.chunks, self.unanswerable)\n",
    "        return msg\n",
    "    \n",
    "    def reset(self):\n",
    "        self.chunks = []\n",
    "        self.chunks_cached = []\n",
    "        self.unanswerable = []\n",
    "    \n",
    "    def _classify_query(self, messages) -> bool:\n",
    "        self.logger.info(\"Starting Query Classification\")\n",
    "        msg = [classify_sysprompt] + messages\n",
    "        response: bool = self.response_func(msg, BooleanModel)\n",
    "        self.logger.info(\"Query Classification completed\")\n",
    "        return response\n",
    "    \n",
    "    def _qualify_existing_pairs(self, messages):\n",
    "        self.logger.info(\"Starting Query-context pairs qualification\")\n",
    "        relevant_pairs, irrelevant_pairs = qualify_existing_pairs(self.chunks, messages, self.response_func, self.max_length_per_split)\n",
    "        relevant_pairs_cached, irrelevant_pairs_cached = qualify_existing_pairs(self.chunks_cached, messages, self.response_func, self.max_length_per_split)\n",
    "        unanswerable_queries, _ = qualify_existing_pairs(self.unanswerable, messages, self.response_func, self.max_length_per_split)\n",
    "        \n",
    "        self.chunks = relevant_pairs + relevant_pairs_cached\n",
    "        self.chunks_cached = irrelevant_pairs + irrelevant_pairs_cached\n",
    "        self.unanswerable = unanswerable_queries\n",
    "        \n",
    "        self.logger.info(\"Query-context pairs qualification completed\")\n",
    "    \n",
    "    def _generate_new_queries(self, messages) -> List[str]:\n",
    "        self.logger.info(\"Starting generation of new queries\")\n",
    "        msg = [new_query_sysprompt] + messages + [new_query_prompt(self.chunks, self.unanswerable)]\n",
    "        new_queries: List[str] = self.response_func(msg, ListStrModel)\n",
    "        self.logger.info(f\"Generated {len(new_queries)} new queries\")\n",
    "        return new_queries\n",
    "    \n",
    "    def _generate_hyde(self, new_queries) -> List[dict]:\n",
    "        self.logger.info(\"Starting HyDE generation\")\n",
    "        query_hyde: List[dict] = [get_HyDE(x, self.response_func) for x in new_queries]\n",
    "        self.logger.info(\"HyDE generation completed\")\n",
    "        return query_hyde\n",
    "    \n",
    "    def _retrieve(self, new_queries, query_hyde) -> List[List[str]]:\n",
    "        self.logger.info(\"Starting retrieval process\")\n",
    "        retrieve_queries: List[str] = self._retrieve_queries(new_queries, query_hyde)\n",
    "        retrieve_embeddings = [self.embedder(query) for query in retrieve_queries]\n",
    "        retrieved_unranked: List[List[str]] = [self.retriever(embedded_query) for embedded_query in retrieve_embeddings]\n",
    "        self.logger.info(\"Retrieval process completed\")\n",
    "        return retrieved_unranked\n",
    "    \n",
    "    def _rerank(self, retrieve_queries, retrieved_unranked) -> List[dict]:\n",
    "        self.logger.info(\"Starting reranking process\")\n",
    "        retrieved_ranked = [self.reranker(query, chunks) for query, chunks in zip(retrieve_queries, retrieved_unranked)]\n",
    "        retrieved_ranked_top_k = [[retrieved_unranked[i][1] for i in ranked_chunks[:self.top_k_rerank]] for ranked_chunks in retrieved_ranked]\n",
    "        new_pairs = [self._pair_factory(query, context) for query, context in zip(retrieve_queries, retrieved_ranked_top_k)]\n",
    "        self.logger.info(\"Reranking process completed\")\n",
    "        return new_pairs\n",
    "    \n",
    "    def _qualify_all_new_generated_pairs(self, new_pairs):\n",
    "        self.logger.info(\"Starting qualification of generated pairs\")\n",
    "        new_pair_qualify_bool = [self._qualify_generated_pairs(pair) for pair in new_pairs]\n",
    "        new_pairs_qualified = [pair for pair, qual in zip(new_pairs, new_pair_qualify_bool) if qual]\n",
    "        new_unanswerables = [pair[\"query\"] for pair, qual in zip(new_pairs, new_pair_qualify_bool) if not qual]\n",
    "        self.logger.info(\"Qualification of generated pairs completed\")\n",
    "        \n",
    "        self.chunks.extend(new_pairs_qualified)\n",
    "        self.unanswerable.extend(new_unanswerables)\n",
    "    \n",
    "    def _qualify_generated_pairs(self, pair):\n",
    "        msg = [qualify_generated_sysprompt] + [qualify_retrieved_prompt(pair[\"query\"], pair[\"context\"])]\n",
    "        qual: bool = self.response_func(msg, BooleanModel)\n",
    "        return qual\n",
    "    \n",
    "    @staticmethod\n",
    "    def _retrieve_queries(queries, hydes) -> List[str]:\n",
    "        msg_list = []\n",
    "        for query, hyde in zip(queries, hydes):\n",
    "            msg = f\"Query: {query}\"\n",
    "            if hyde[\"generate\"]:\n",
    "                msg = msg + f\"Hypothetical Answer: {hyde[\"response\"]}\"\n",
    "            msg_list.append(msg)\n",
    "        return msg_list\n",
    "        \n",
    "    @staticmethod\n",
    "    def _pair_factory(query: str, context: List[str]):\n",
    "        pair = {\n",
    "            \"query\": query,\n",
    "            \"context\": context\n",
    "        }\n",
    "        return pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Test only) ChatUI\n",
    "\n",
    "This is for demonstration only (a web client will be used in production)\n",
    "\n",
    "TODO: Replace the dummy_inference() function with our own inference function after that is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "class DemoChatUI:\n",
    "    def __init__(self, response_inference_function: Callable, pool_of_queries: PoolOfQueries, inference_sysprompt: dict):\n",
    "        self.response_inference_function = response_inference_function\n",
    "        self.pool_of_queries = pool_of_queries\n",
    "        self.inference_sysprompt = inference_sysprompt\n",
    "        self.message_history = []\n",
    "\n",
    "        self.output = widgets.Output()\n",
    "        self.text_input = widgets.Text(placeholder='Type your message here...')\n",
    "        self.send_button = widgets.Button(description='Send')\n",
    "        self.role_checkbox = widgets.Checkbox(description='Show roles', value=False)\n",
    "        self.reset_button = widgets.Button(description='Reset Chat')\n",
    "        \n",
    "        self.send_button.on_click(self.on_send)\n",
    "        self.text_input.on_submit(self.on_send)\n",
    "        self.role_checkbox.observe(self.update_chat_display, names='value')\n",
    "        self.reset_button.on_click(self.reset_chat)\n",
    "        \n",
    "        input_box = widgets.HBox([self.text_input, self.send_button])\n",
    "        input_box.layout.display = 'flex'\n",
    "        self.text_input.layout.flex = '1'\n",
    "        \n",
    "        bottom_box = widgets.HBox([self.reset_button, self.role_checkbox])\n",
    "        bottom_box.layout.display = 'flex'\n",
    "        bottom_box.layout.justify_content = 'space-between'\n",
    "        \n",
    "        self.chat_box = widgets.VBox([self.output, input_box, bottom_box])\n",
    "        self.main_output = widgets.Output()\n",
    "        \n",
    "        with self.main_output:\n",
    "            display(self.chat_box)\n",
    "        \n",
    "        display(self.main_output)\n",
    "        \n",
    "    def on_send(self, _):\n",
    "        user_message = self.text_input.value\n",
    "        if user_message.strip():\n",
    "\n",
    "            current_msg = self.make_user_message_dict(user_message)\n",
    "            self.message_history.append(current_msg)\n",
    "            self.pool_of_queries.update(self.message_history)\n",
    "            self.text_input.value = ''\n",
    "            \n",
    "            current_context_msg = self.pool_of_queries.current_context_msg()\n",
    "            new_msg = self.response_inference_function([self.inference_sysprompt] + [current_context_msg] + self.message_history)\n",
    "            self.message_history.append(new_msg)\n",
    "            \n",
    "            self.update_chat_display()\n",
    "            \n",
    "    def add_message(self, role, content):\n",
    "        self.message_history.append({\"role\": role, \"content\": content})\n",
    "        self.update_chat_display()\n",
    "        \n",
    "    def update_chat_display(self, _=None):\n",
    "        self.output.clear_output()\n",
    "        with self.output:\n",
    "            for message in self.message_history:\n",
    "                role = message['role']\n",
    "                content = message['content']\n",
    "                \n",
    "                if role == 'user':\n",
    "                    align = 'right'\n",
    "                    color = '#DCF8C6'\n",
    "                elif role == 'assistant':\n",
    "                    align = 'left'\n",
    "                    color = '#E5E5EA'\n",
    "                else:\n",
    "                    align = 'left'\n",
    "                    color = '#F3E5F5'\n",
    "                \n",
    "                role_display = f\"<small>{role}: </small>\" if self.role_checkbox.value else \"\"\n",
    "                \n",
    "                display(HTML(f\"\"\"\n",
    "                    <div style=\"text-align: {align};\">\n",
    "                        <div style=\"display: inline-block; background-color: {color}; padding: 5px 10px; border-radius: 10px; max-width: 70%;\">\n",
    "                            {role_display}{content}\n",
    "                        </div>\n",
    "                    </div>\n",
    "                \"\"\"))\n",
    "\n",
    "    def reset_chat(self, _):\n",
    "        self.message_history = []\n",
    "        self.text_input.value = ''\n",
    "        self.role_checkbox.value = False\n",
    "        self.pool_of_queries = type(self.pool_of_queries)()  # Create a new instance of the same type\n",
    "        self.update_chat_display()\n",
    "\n",
    "    @staticmethod\n",
    "    def make_user_message_dict(new_message: str):\n",
    "        return {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": new_message\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We retrieve more chunks then we acturally provide the LLM\n",
    "top_k_retrieved = 10\n",
    "top_k_reranked = 3\n",
    "\n",
    "# Models to use\n",
    "embedding_model_name = \"dunzhang/stella_en_1.5B_v5\" # Model must be supported by Transformer model\n",
    "reranker_model_name = \"castorini/monot5-base-msmarco-10k\" # Must be a T5 model, supported by the pygaggle library\n",
    "\n",
    "# Pool of Queries\n",
    "# Set to 20 if need logging and set to 51/ leave blank if no need logging\n",
    "poq_log_level = 20 \n",
    "\n",
    "# Retrieve Function padding\n",
    "# For example, if the padding is [0.5, 0.25], the retrieve function will pad the retrieved chunk (say, it has index i) \n",
    "# with the later half (0.5) of the previous chunk (index i-1) at the front\n",
    "# and the first quarter (0.25) of the next chunk (index i+1) at the back\n",
    "padding = [0.5, 0.5]\n",
    "\n",
    "# Search Parameters for Milvus, TODO: Change to HSNW search scheme\n",
    "# Use inner product because embeddings are normalized\n",
    "milvus_search_params = {\n",
    "    \"metric_type\": \"IP\", \n",
    "    'params': {\n",
    "        'level': 2,\n",
    "    }\n",
    "}\n",
    "\n",
    "embedding_args = {\n",
    "    \"device\": \"cuda\",\n",
    "    \"normalize_embeddings\": True\n",
    "}\n",
    "\n",
    "# arguement for Groq\n",
    "groq_args = {\n",
    "    \"model\": \"llama-3.1-70b-instant\",\n",
    "    \"max_tokens\": 8192,\n",
    "    \"temperature\": 0.5, # To tune\n",
    "}\n",
    "\n",
    "groq_args_structured = {\n",
    "    \"model\": \"llama-3.1-8b-instant\",\n",
    "    \"max_tokens\": 8192,\n",
    "    \"temperature\": 0.2, # To tune\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from groq import Groq\n",
    "from pymilvus import connections, Collection\n",
    "from functools import partial\n",
    "from pygaggle.rerank.transformer import MonoT5\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# File Path\n",
    "documents_path = os.path.join(os.getcwd(), \"documents\") # For demo purposes can only assume \"documents\" is in root directory\n",
    "\n",
    "# Setting up VectorDB (milvus)\n",
    "# The collection's schema \"schema\" is defined in the DB portion of the field\n",
    "connections.connect(host='localhost', port='19530')\n",
    "collection = Collection(name=\"documents\", schema=schema)\n",
    "\n",
    "\n",
    "# Preparing embedding function\n",
    "embedding_model = SentenceTransformer(embedding_model_name, trust_remote_code=True).cuda()\n",
    "embedding_function = partial(get_embeddings, embedding_model = embedding_model, **embedding_args)\n",
    "\n",
    "\n",
    "# Preparing reranker function (This is wrong, will work on it later)\n",
    "reranker = MonoT5(reranker_model_name)\n",
    "rerank_function = partial(rerank_results, top_k = top_k_reranked, monoT5reranker = reranker)\n",
    "\n",
    "# Preparing inference functions\n",
    "groq_client = Groq(api_key = GROQ_API_KEY)\n",
    "response_inference_function = partial(get_response, client = groq_client, groq_args = groq_args)\n",
    "response_structured_function = partial(get_structured_response, client = groq_client, groq_args = groq_args_structured)\n",
    "\n",
    "# Retriever function\n",
    "retrieve_function = partial(retrieve, \n",
    "                            top_k_retrieved = top_k_retrieved,\n",
    "                            collection = collection,\n",
    "                            search_params = milvus_search_params)\n",
    "\n",
    "\n",
    "# Initialize Pool of queries\n",
    "pool_of_queries = PoolOfQueries(embedding_function = embedding_function,\n",
    "                                rerank_function = rerank_function,\n",
    "                                retrieve_function = retrieve_function,\n",
    "                                response_function = response_structured_function,\n",
    "                                log_level = poq_log_level) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Workflow\n",
    "documents = read_directory(documents_path, recursive = True)\n",
    "store_and_embed_documents(documents, collection, embedding_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ui = DemoChatUI(response_inference_function=response_inference_function,\n",
    "                pool_of_queries=pool_of_queries,\n",
    "                inference_sysprompt=inference_sysprompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
